---
title: 统计学习方法--支持向量机
date: 2019-07-12 16:49:37
tags: [统计学习,机器学习]
categories: 大数据与网络安全
---

## 支持向量机概述
支持向量机是一种二类分类模型，它的基本模型是定义在特征空间上的间隔最大的线性分类器。支持向量机由简至繁依次可分为：线性可分支持向量机，线性支持向量机，非线性支持向量机。当数据线性可分时，通过硬间隔最大化的约束来学习一个分类器，称为线性可分支持向量机；当数据近似线性可分时，通过软间隔最大化的约束来学习一个分类器，称为线性支持向量机；当数据线性不可分时通过使用核技巧及软间隔最大化，学习非线性支持向量机。故此，支持向量机可以简化的描述为通过训练数据集来寻找一个能够将数据正确分类的超平面，此分离超平面满足数据集中的数据点距此超平面的间隔（软间隔/硬间隔）最大化。

## 基础前提

### 函数间隔和几何间隔
间隔最大化是贯穿于整个支持向量机模型的最核心的部分。一般来说，一个点距离分离超平面的远近可以表示分类预测的确信程度，在分离超平面确定的情况下，若一个点离超平面越远则分类正确的可信度越高。故此，支持向量机寻求间隔最大化的意义在于分离超平面不仅可以正确分类数据点，而且对于最难分的实例（距离超平面最近的点）也有较大的分类可信度，因此该模型再对未知数据分类时应该也具有较好的性能。

#### 函数间隔
> 对于给定的训练数据集$T$和超平面$(\omega,b)$，
> 
> 定义超平面$(\omega,b)$关于样本点$(x_i,y_i)$的函数间隔为$$\hat{\gamma_i}=y_i(\omega\cdot x_i+b)$$
> 定义超平面$(\omega,b)$关于训练数据集$T$的函数间隔为超平面$(\omega,b)$关于$T$中所有样本点$(x_i,y_i)$的函数间隔最小值，即$$\hat{\gamma}=\min_{i=1,2,..,N}\hat{\gamma_i}$$

#### 几何间隔
由于函数间隔在当$\omega$和$b$成比例的改变时候，得到的分离超平面并没有变化，但是函数间隔却也成比例的改变，故此自然想到应该对其进行规范化处理，令$||\omega||=1$，这样就得到了几何间隔的定义。

> 对于给定的训练数据集$T$和超平面$(\omega,b)$
> 
> 定义超平面$(\omega,b)$关于样本点$(x_i,y_)i$的几何间隔为$$\gamma_i=y_i(\frac{\omega}{||\omega||}\cdot x_i+\frac{b}{||\omega||})$$
> 定义超平面$(\omega,b)$关于训练数据集$T$的几何间隔为超平面$(\omega,b)$关于$T$中所有样本点$(x_i,y_i)$的几何间隔最小值，即$$\gamma=\min_{1,2,...,N}\gamma_i$$

## 线性可分支持向量机与硬间隔最大化

给定线性可分训练数据集，通过间隔最大化(几何间隔最大化)或等价的求解相应的凸二次规划问题得到的分离超平面为$\omega^*\cdot x+b^*=0$,相应的分类决策函数为$f(x)=sign(\omega^*\cdot x+b^*)$。线性可分支持向量机可以表述为下列形式：$$\max_{\omega,b}\gamma$$ $$s.t. y_i(\frac{\omega}{||\omega||}\cdot x_i+\frac{b}{||\omega||})\geq\gamma,i=1,2...,N$$即转化为了一个约束最优化问题。依据函数间隔和几何间隔的关系及约束最优化的求解方式，可以将上述求解最大值的问题转化为求解最小值的问题，即：$$\min_{\omega,b}\frac{1}{2}||\omega||^2$$ $$s.t. y_i(\omega\cdot x_i+b)-1\geq 1,i=1,2...,N$$

故此可以有线性可分支持向量机算法为：

> 输入：线性可分训练数据集$T=\{(x_1,y_2),(x_2,y_2),...,(x_N,y_N)\}$,其中$x_i\in X=R^n,y_i\in Y=\{+1,-1\},i=1,2,...,N$

> 输出：最大间隔分离超平面及分类决策函数

> (1) 构造并求解约束最优化问题：$$\min_{\omega,b}\frac{1}{2}||\omega||^2$$ $$s.t. y_i(\omega\cdot x_i+b)-1\geq 1,i=1,2...,N$$得到最优解$\omega^*$,$b^*$
> (2) 由此得到分离超平面：$$\omega^* \cdot x+b^* = 0$$,分类决策函数$$f(x)=sign(\omega^* \cdot x+b^*)$$

## 线性支持向量机与软间隔最大化
当数据集线性不可分时表明对于数据集中的一些实例，上述约束方法中的约束不等式不能都成立，即存在一些实例，无法满足函数间隔大于等于1的约束要求。为了解决这个问题，可以对每个样本点引入一个松弛变量$\zeta_i\geq0$,使得函数间隔加上松弛变量大于等于1，即约束条件放宽为:$y_i(\omega\cdot x_i+b)\geq 1-\zeta_i$,同时对每个松弛变量支付一个代价$\zeta_i$。目标函数则变为$\frac{1}{2}||\omega||^2+C \sum_{i=1}^{N}\zeta_i$，这里$C>0$称为惩罚参数，$C$值大时对误分类的惩罚增大，$C$值小时对误分类的惩罚小。故由此可得到线性支持向量机算法的原始表达形式为：$$\min_{\omega,b,\zeta} \frac{1}{2}||\omega||^2+C\sum_{i=1}^N\zeta_i$$ $$s.t. y_i(\omega\cdot x_i+b)\geq 1-\zeta_i,i=1,2,...,N$$ $$\zeta_i\geq 0,i=1,2,...,N$$这样根据原始问题求解得到$\omega^*$,$b^*$即可得到分离超平面以及分类决策函数
## 非线性支持向量机与核函数
对于线性分类问题，线性支持向量机是一种非常有效的方法。但是有时要解决的是非线性问题，此时就需要收盘每个非线性支持向量机，其重点在于利用核技巧。核技巧不仅应用于支持向量机，而且也可以应用于其他统计学问题中。核技巧就是通过首先使用一个变换将原空间的数据映射到新的空间，然后在新空间里用线性分类学习方法从训练数据中学习分类模型。

核技巧的基本想法是通过一个非线性变换将输入空间（欧式空间$R^n$或离散集合）对应于一个特征空间（希尔伯特空间$\mathrm{H}$），使得在输入空间$R^n$中的超曲面模型对应于特征空间$\mathrm{H}$中的超平面模型（支持向量机）。这样，分类问题的学习任务通过在特征空间中求解线性支持向量机就可以完成。

### 核函数

> 设$\chi$是输入空间（欧式空间$R^n$的子集或离散集合），又设$\mathrm{H}$为特征空间（希尔伯特空间），如果存在一个从$\chi$到$\mathrm{H}$的映射$$\phi(x):\chi \to \mathrm{H}$$使得对所有$x,z\in\chi$,函数$K(x,z)$满足条件$$K(x,z)=\phi(x)\cdot\phi(z)$$则称$K(x,z)$为核函数，$\phi(x)$为映射函数，式中$\phi(x)\cdot\phi(z)$为$\phi(x)$和$\phi(z)$的內积

### 非线性支持向量机

从非线性分类训练集，通过核函数与软间隔最大化学习得到的分类决策函数$$f(x)=sign(\sum_{i=1}^N\alpha_i^*y_iK(x,x_i)+b^ *)$$称为非线性支持向量机，$K(x,z)$是正定核函数

> 输入：训练数据集$T=\{(x_1,y_1),(x_2,y_2),...,(x_N,y_N)\}$其中$x_i\in\chi=R^n,y_i\in Y=\{-1,+1\},i=1,2,...,N$

> 输出：分类决策函数

> (1) 选取适当的核函数$K(x,z)$和适当的参数$C$，构造并求解最优化问题$$\min_\alpha \frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_j y_iy_jK(x_i,x_j)-\sum_{i=1}^N\alpha_i$$ $$s.t. \sum_{i=1}^N\alpha_iy_i=0$$ $$0\leq\alpha_i\leq C,i=1,2,...,N$$求出最优解$\alpha^*=(\alpha_1^*,\alpha_2^*,...,\alpha_N^*)^T$
> (2) 选择$\alpha^*$的一个正分量$0<\alpha_j^*<C$，计算$$b^*=y_j-\sum_{i=1}^N\alpha_i^ *y_iK(x_i,x_j)$$
> (3) 构造决策函数：$$f(x)=sign(\sum_{i=1}^N\alpha_i^*y_iK(x,x_i)+b^ *)$$