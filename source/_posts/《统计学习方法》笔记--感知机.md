---
title: 统计学习方法--感知机
date: 2019-05-28 16:49:37
tags: [统计学习,机器学习]
categories: 大数据与网络安全
---

## 感知机概述
感知机是二类分类的线性模型，输入为实例的特征向量，输出为实例的类别，取+1和-1两个值。感知机本质对应于输入空间的一个超平面，通过将正负两类通过一个超平面划分开来，属于判别式模型。

## 感知机模型

> 假设输入空间（特征空间）是 $\chi \subseteqq R^n$ 输出空间是 $y=\{+1,-1\}$ 。输入$x\in \chi$表示实例的特征向量，对应于输入空间的点；输出$ y \in Y$表示实例的类别。由输入空间到输出空间的如下函数 $$ f(x)=sign(w \cdot x +b)$$ 称为感知机。其中 $w$ 和$b$称为感知机模型参数， $w\in R^n$ 叫做权值或者权重向量，$b\in R$ 叫做偏置，$w\cdot x$ 表示$w$和$x$的内积，$sign$是符号函数,即$$sign(x)=\left\{\begin{array}{cc}+1, & x \geqslant0 \\-1, & \ x<0\end{array}\right.$$

## 感知机学习策略

### 数据集的要求

运用感知机模型首先要求数据集是线性可分的，即通俗的说就是对于给定的数据集，存在这样一个超平面，能够将数据集的正实例点(y=+1)和负实例点(y=-1)完全正确的划分到超平面的两侧，这样就称为数据集是线性可分的。

#### 形式化定义：
> 给定一个数据集，$$T=\{(x_1,y_1),(x_2,y_2)...(x_N,y_N)\} $$其中 $x_i \in \chi=R^n,y_i \in \gamma=\{ +1,-1\},i=1,2,...N,$如果存在某个超平面S,对所有$y_i=+1$的实例$i$有$w \cdot x_i+b>0$,对所有$y_i=-1$的实例$i$有$w \cdot x_i+b<0$。则称数据集T为线性可分数据集，否则为线性不可分。

### 学习策略（损失函数最小化策略）
感知机$sign(w\cdot x+b)$学习的损失函数定义为：$$L(w,b)=-\sum_{x_i \in M}y_i(w\cdot x+b)$$其中$M$为误分类点的集合。

这个损失函数的基本思想就是通过衡量误分类点距离超平面的总距离来计算，实际上计算了感知机器学习的经验风险函数。直观来看，如果没有误分类点，损失函数就是0，而且误分类点越少，离超平面越近。损失函数值就越少。

> 注：任意一点到超平面的距离公式为$$\frac{1}{||w||}|w\cdot x+b|$$这里$||w||$是$w$的$L_2$范数。在构造感知机学习损失函数时不考虑$\frac{1}{||w||}$

## 感知机学习算法

感知机学习算法有一个直观的理解，当一个实例被误分类时其位于分离超平面错误的一侧，我们通过调整$w，b$的值使得超平面向该误分类点的一侧移动，以减少该误分类点与超平面的距离，直到超平面越过该点，使其被正确分类。


### 原始形式

> 输入：训练数据集$T=\{(x_1,y_1),(x_2,y_2)...(x_N,y_N)\};$其中 $x_i \in \chi=R^n,y_i \in \gamma=\{ +1,-1\},i=1,2,...N$;学习率$\eta(0<\eta\leqslant1)$

> 输出：$w,b;$感知机模型$f(x)=sign(w\cdot x+b)$

> (1)选取初始值$w_0,b_0$
> (2)在训练集中选取数据$(x_i,y_i)$
> (3)如果$y_i(w\cdot x_i+b)\leqslant0$,$$w\gets w+\eta y_ix_i$$ $$ b\gets b+\eta y_i$$
> (4)转至(2)，直至训练集中没有误分类点

### 对偶形式

> 输入：线性可分数据集$T=\{(x_1,y_1),(x_2,y_2)...(x_N,y_N)\};$其中 $x_i \in \chi=R^n,y_i \in \gamma=\{ +1,-1\},i=1,2,...N$;学习率$\eta(0<\eta\leqslant1)$

> 输出：$\alpha,b;$感知机模型$f(x)=sign(\sum_{j=i}^{N}\alpha_jy_jx_j\cdot x+b)$,其中$\alpha = (\alpha_1,\alpha_2,...\alpha_N)^T$

> (1)$\alpha\gets0,b\gets0$
> (2)在训练数据集中选取数据$(x_i,y_i)$
> (3)如果$y_i(\sum_{j=1}^{N}\alpha_jy_jx_j\cdot x_i+b)\leqslant0$,$$ \alpha_i\gets\alpha_i+\eta$$ $$ b\gets b+\eta y_i$$
> (4)转至(2)直至没有误分类数据

### 联系与区别

在最开始看到两种形式的感知机学习算法时有点不明白为什么会在原始形式上发展出对偶形式，因为虽然对偶形式的算法可以通过提前计算一些变量来获得一定的加速，但是本质上还是要迭代更新权重来进行训练。

后来明白这是由于从不同的角度去解决问题而给出的算法，对偶通俗理解即从不同角度去解答相似问题，但问题的解是相通的，甚至是一样的。另外感知机的对偶算法在特征维度很高时提升性能的效果很明显，由于可以提前计算出$x_j\cdot y_j$这样每次在判断误分类时可以直接查找表中数据，不用重新计算两者内积。例如对于特征向量维度为N的数据集，可以把每次的$N\cdot N$运算降低到$N$
详细可以阅读知乎回答[如何理解感知机学习算法的对偶形式？](https://www.zhihu.com/question/26526858/answer/131591887)
