---
title: 统计学习方法--逻辑斯谛回归
date: 2019-07-12 16:49:37
tags: [统计学习,机器学习]
categories: 大数据与网络安全
---

## 逻辑斯谛回归概述
逻辑斯谛回归的主要思想是：根据现有数据对分类边界线建立回归公式，以此进行分类。这里的“回归”一词源于最佳拟合，表示要找到最佳拟合参数。而最佳拟合参数就是在训练分类器时，通过最优化算法获得。
### 逻辑斯谛分布
> 设$X$是连续随机变量，$X$服从逻辑斯谛分布是指$X$具有以下分布函数和概率密度函数：$$F(x)=P(X\leq x)=\frac{1}{1+e^{-(x-\mu)/\gamma}}$$ $$ f(x)=F'(x)=\frac{e^{-(x-\mu)/\gamma}}{\gamma(1+e^{-(x-\mu)/\gamma})^2}$$其中$\mu$为位置参数，$\gamma \gt 0$为形状参数

### 二项式逻辑斯谛回归模型
> $$P(Y=1|x) = \frac{exp(\omega\cdot x+b)}{1+exp(\omega\cdot x+b)}$$$$P(Y=0|x) = \frac{1}{1+exp(\omega\cdot x+b)}$$
> 这里$x\in R^n$是输入，$Y\in\{0,1\}$是输出，$\omega\in R^n$和$b\in R^n$是参数，$\omega$称为权值向量，$b$称为偏置，$\omega\cdot x$是$\omega$和$X$的內积

> 对于给定的输入实例$X$，按照上式可以求得实例属于两种类别的概率，逻辑斯谛回归模型比较两个概率的大小，将实例归入概率较大的那一类中。

### 模型参数估计
> 对于给定的训练数据集$T=\{(x_1,y_1),(x_2,y_2)...(x_N,y_N)\}$其中$x_i\in R^n,y_i\in \{0,1\}$可以应用`极大似然估计`来得到模型的参数，从而得到逻辑斯谛回归模型。

> 设$$P(Y=1|x)=\pi(x),P(Y=0|x)=1-\pi(x)$$则可得似然函数$$\prod_{i=1}^N[\pi(x_i)]^{y_i}[1-\pi(x_i)]^{1-y_i}$$对数似然函数为：$$L(\omega)=\sum_{i=1}^N[y_i\log\pi(x)+(1-y_i\log(1-\pi(x_i)))]$$即最终变成求解$L(\omega)$极大值的问题。一般可以采用`梯度下降`或者是`拟牛顿法`求解

> 最后求出$\omega$的极大似然估计值即可得到逻辑斯谛回归模型。

### 多项式逻辑斯谛回归

二项式逻辑斯谛回归应用于二类分类问题，将其推广到多项式逻辑斯谛回归就可以应用于多类分类问题。

> 假设离散型随机变量$Y$的取值集合是$\{1,2,...,K\}$则多项式逻辑斯谛回归模型是$$P(Y=k|x)=\frac{\exp(\omega_k\cdot x)}{1+\sum_{k=1}^{K-1}\exp(\omega_k \cdot x)},  k=1,2,3...,K-1$$ $$P(Y=K|x)=\frac{1}{1+\sum_{k=1}^{K-1}\exp(\omega_k \cdot x)}$$这里$x\in R^{n+1},\omega_k\in R^{n+1}$这是将偏置项拓展到权值向量$\omega$和输入向量$x$中。

### 逻辑斯谛回归模型和线性回归模型，SVM模型的异同
总体而言，三者都属于线性模型，只是通过计算得到的线性平面的用法不同。逻辑斯谛回归和SVM是分类模型，线性回归属于回归模型。

#### 关于三种模型异同的相关文章：
- [线性模型（线性回归、感知机和逻辑斯谛回归）](https://blog.csdn.net/weixin_34032792/article/details/87134490)
- [SVM简介、SVM与感知机、逻辑回归LR的区别](https://blog.csdn.net/sunflower_sara/article/details/81713449)
- [逻辑斯蒂回归和感知机模型、支持向量机模型对比](https://blog.csdn.net/zrh_CSDN/article/details/80920329)