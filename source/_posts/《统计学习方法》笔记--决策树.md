---
title: 统计学习方法--决策树
date: 2019-06-16 16:49:37
tags: [统计学习,机器学习]
categories: 大数据与网络安全
---

## 决策树概述
决策树模型呈树形结构，在分类过程中表示基于特征对实例进行分类的过程。决策树模型可以视为if-then规则的集合，也可以视为是定义在特征空间与类别空间上的条件改了分布。主要优点是模型具有很好的可解释性，分类速度快，缺点是构建决策树时用的特征序列对分类效果有较大的影响。决策树学习过程通常包括三个步骤：**特征选择**，**决策树的生成**，**决策树的修剪**

## 决策树模型
分类决策树模型是一种描述对实例进行分类的树形结构。决策树由节点和有向边组成。节点包括两种类型：内部节点和叶节点。内部节点表示一个特征或属性，叶节点表示一个类。

决策树学习的本质是从给定的训练数据集中依据属性或特征归纳出一组分类规则。与给定的训练数据集相符合的分类规则可能有多个，决策树模型就是需要从多个符合的分类规则中找到损失最小的，泛化能力最好的一组分类规则。决策树常用的损失函数是正则化的极大似然函数，决策树的学习策略是以损失函数为目标函数的最小化。由于所有可能的决策树组成的解空间较大，从中找到最优的决策树是NP完全问题，因此一般多采用启发式算法来近似求解。

决策树学习的算法通常是一个递归的选择最优特征的过程。从可选的特征集合中选出最优的特征（即依据该特征能最有效的将训练数据集分类），按照这一特征将数据集分割成子集，该特征作为这些子集的根节点。如果这些子集已经基本可以正确分类，那么构建叶节点，并将这些子集分到所对应的的叶节点中去。如果还有子集不能被正确分类，那么对这些子集选取新的最优特征，继续对其进行分割，直至所有训练数据都被正确分类。至此就构建出来一颗决策树。

通过上述步骤构建的决策树可以对训练数据集进行很好的分类，但是并不一定有很好的泛化能力，即可能发生过拟合现象。为了增强其泛化能力，我们需要对构建好的决策树进行自底向上的剪枝，即将树变得更简单一点。通过去掉决策树中过于细分的叶节点，使其回退到父节点甚至更高节点，用父节点或更高节点作为新的叶节点。

### 特征选择
特征选择在于选取对训练数据具有良好分类能力的特征，这样可以提高决策树的学习效率。通常特征选择的准则是信息增益或信息增益比。

#### 熵
在信息论和概率统计中，熵是表示随机变量不确定性的度量。设$X$是一个取有限个值得离散随机变量，其概率分布为:$$P(X=x_i)=p_i, i=1,2,3...,n$$,则随机变量$X$的熵定义为:$$H(X)=-\sum_{i=1}^np_i\log p_i$$或(从定义可知和X的取值无关，只和其分布有关)$$H(p)=-\sum_{i=1}^np_i\log p_i$$熵有两种单位：（1）当公式中的对数以2为底时，单位为比特(bit)；（2）当公式中的对数以e为底时，单位为纳特(nat)。**熵越大，随机变量的不确定性就越大。**

#### 条件熵
设有随机变量$(X,Y)$其联合概率分布为:$$P(X=x_i,Y=y_j)=p_{ij},i=1,2,...n,j=1,2,...m$$条件熵$H(Y|X)$表示在已知随机变量$X$的条件下随机变量$Y$的不确定性，定义为$X$给定条件下$Y$的条件概率分布的熵对$X$的数学期望：$$H(Y|X)=\sum_{i=1}^np_iH(Y|X=x_i)$$ $$p_i=P(X=x_i),i=1,2,...,n$$

#### 经验熵和经验条件熵
当熵和条件熵中的概率由数据估计（特别是极大似然估计）得到时，对应的熵与条件熵分别称为**经验熵**和**经验条件熵**若有概率为0的，则令$0\log0=0$

#### 信息增益（互信息）
信息增益（也称为互信息）表示得知特征$X$的信息后特征$Y$的信息不确定性减少的程度，反应了特征$X$对于其他特征不确定性的影响程度。

> 特征$A$对训练数据集$D$的信息增益$g(D,A)$定义为集合$D$的经验熵$H(D)$与特征$A$在给定条件下$D$的经验条件熵$H(D|A)$之差，即$$g(D,A)=H(D)-H(D|A)$$

**信息增益大的特征具有更强的分类能力（即表示在已知该特征的情况下，整个集合的不确定降低最多）**

#### 信息增益比
以信息增益作为选择特征的准则，存在偏向于选择取值较多的特征的问题。即当一个特征可能的取值较多时，其计算出来的信息增益可能会较高，但是并不一定就一定是一个更有效的分类特征。采用信息增益比可以对这一问题进行校正，这是特征选择的另一准则。

> 特征$A$对训练数据集$D$的信息增益比$gR(D,A)$定义为其信息增益$g(D,A)$与训练数据集$D$关于特征$A$的值的熵$H_A(D)$之比，即：$$gR(D,A)=\frac{g(D,A)}{H_A(D)}$$,其中,$H_A(D)=-\sum_{i=1}^n\frac{|D_i|}{|D|}\log_2\frac{|D_i|}{|D|},n$是特征$A$取值的个数

### 决策树生成

#### ID3算法(基于信息增益)
ID3算法的核心是在决策树各个节点上应用信息增益准则选择特征，递归的构建决策树。

> 输入：训练数据集$D$，特征集$A$阈值$\epsilon$

> 输出：决策树$T$

>  (1)若$D$中实例属于同一类$C_k$，则$T$为单节点树。并将类$C_k$作为该节点的类标记，返回$T$
>  (2)若$A=\phi$，则$T$为单节点树，并将$D$中实例数最大的类$C_k$作为该节点的类标记，返回$T$
>  (3)否则，按照求解**信息增益**的算法，计算$A$中各特征对$D$的信息增益，选择**信息增益**最大的特征$A_g$
>  (4)如果$A_g$的**信息增益**小于阈值$\epsilon$,则置$T$为单节点树，并将$D$中实例数最大的类$C_k$作为该节点的类标记，返回$T$
>  (5)否则，对$A_g$的每一可能值$a_i$,依$A_g=a_i$将$D$分割为若干非空子集$D_i$，将$D_i$中实例数最大的类作为标记，构建子节点，由节点及其子节点构成树$T$,返回$T$
>  (6)对第$i$个子节点，以$D_i$为训练集，以$A-\{A_g\}$为特征集，递归调用步骤(1)~(5)，得到子树$T_i$,返回$T_i$


#### C4.5算法(基于信息增益比)
C4.5算法本质上和ID3算法是一样的，只是采用信息增益比作为特征选择的评价准则。

> 输入：训练数据集$D$，特征集$A$阈值$\epsilon$

> 输出：决策树$T$

>  (1)若$D$中实例属于同一类$C_k$，则$T$为单节点树。并将类$C_k$作为该节点的类标记，返回$T$
>  (2)若$A=\phi$，则$T$为单节点树，并将$D$中实例数最大的类$C_k$作为该节点的类标记，返回$T$
>  (3)否则，按照求解**信息增益比**的算法，计算$A$中各特征对$D$的信息增益，选择**信息增益比**最大的特征$A_g$
>  (4)如果$A_g$的**信息增益比**小于阈值$\epsilon$,则置$T$为单节点树，并将$D$中实例数最大的类$C_k$作为该节点的类标记，返回$T$
>  (5)否则，对$A_g$的每一可能值$a_i$,依$A_g=a_i$将$D$分割为若干非空子集$D_i$，将$D_i$中实例数最大的类作为标记，构建子节点，由节点及其子节点构成树$T$,返回$T$
>  (6)对第$i$个子节点，以$D_i$为训练集，以$A-\{A_g\}$为特征集，递归调用步骤(1)~(5)，得到子树$T_i$,返回$T_i$

### 决策树的剪枝

通过前边决策树生成算法的步骤生成的决策树可能对训练数据有很好的拟合效果，但是由于分支过细，可能会包含太多训练集中的信息，导致泛化能力很差，对未知的数据没有准确的分类。解决这个问题的办法是考虑决策树的复杂度，对已生成的决策树进行简化。

> 输入：生成算法产生的整个决策树$T$,参数$\alpha$

> 输出：修剪后的子树$T_\alpha$

> (1)计算每个节点的经验熵
> (2)递归的从树的叶节点向上回缩。设一组叶节点回缩到其父节点之前与之后的整体树分别为$T_B,T_A$，其对应的损失函数分别是$C_\alpha(T_B)$与$C_\alpha(T_A)$,如果$$C_\alpha(T_A)\leq C_\alpha(T_B)$$则进行剪枝，将父节点变为新的叶节点。
> (3)返回(2)，直至不能继续为止，得到损失函数最小的子树$T_\alpha$
### CART算法（分类回归树算法）
分类回归树算法完整的包含了决策树从特征选择，决策树生成，决策树剪枝的完整过程。CART假设决策树为二叉树，通过将内部结点取值按照“是”和“否”来划分数据集，左分支为结点取值为“是”的数据集合，右分支为取值为“否”的结点集合，这样就可以递归的将数据集划分为有限个集合。

#### CART的特征选择
CART的特征选择主要有两种方法：
- 针对回归树，采用平方误差最小化准则来选择特征及特征的切分点；
- 针对分类树，采用基尼指数作为准则来选择最优特征和最优切分点；
##### 基尼指数：
> 分类问题中，假设有$K$个类，样本点属于第$k$类的概率为$p_k$，则概率分布的基尼指数定义为$$Gini(p)=\sum_{k=1}^{K}p_k(1-p_k)=1-\sum_{k=1}^{K}p_{k}^2$$对于给定样本集合$D$，其基尼指数为$$Gini(p)=1-\sum_{k=1}^{K}(\frac{|C_k|}{|D|})^2$$其中$C_k$是$D$中属于第$k$类的样本子集，$K$是类的个数。与熵类似，当基尼指数越大，表明集合的不确定性越大。

在实际使用中，如果样本集合$D$依据特征$A$的可能取值$a$分割为两个子集$D_1$和$D_2$，则此时样本集合$D$的基尼指数定义为$$Gini(D,A)=\frac{|D_1|}{|D|}Gini(D_1)+\frac{|D_2|}{|D|}Gini(D_2)$$

#### CART的生成

##### 回归树
> 输入：训练数据集$D$，停止计算条件

> 输出：回归树$f(x)$

> (1)遍历寻找最优切分变量（最优切分特征）和切分点（最优切分特征的最优切分取值），使得平方误差损失最小，即求解$$\min_{j,s}[\min_{c_1}\sum_{x_i \in R_1(j,s)}(y_i-c_1)^2+\min_{c_2}\sum_{x_i\in R_2(j,s)}(y_i-c_2)^2]$$遍历变量$j$，对固定的切分变量$j$（特征）扫描切分点$s$（特征的取值），选取使得平方误差最小的对$(j,s)$。
> 注：直观的解释就是遍历训练数据集$D$的全部特征，对于每一个特征$A_i$的可能取值$a_i$，计算用特征$A_i$取值为$a_i$时切分数据集后，使整个数据集的平方误差最小的特征以及特征对应的取值$（A_i，a_i）$，也就是公式中的$(j,s)$。在找到使得数据集$D$平方误差最小的特征和特征值后，用该特征和特征值切分数据集。
> (2)使用选定的对$(j,s)$划分区域并计算相应的输出值$$R_1(j,s)=\{x|x^{(j)}\leq s\},R_2(j,s)=\{x|x^{j}>s\}$$ $$\hat{c_m}=\frac{1}{N_m}\sum_{x_i\in R_m(j,s)}y_i,x_\in R_m,m=1,2$$
> (3)继续递归的对两个子区域调用步骤（1）和（2），直到满足停止条件
> (4)将输入数据集划分为M个区域$R_1,R_2...,R_M$,生成决策树：$$f(x)=\sum_{m=1}^{M}\hat{c_m}I(x\in R_m)$$

#### 分类树
> 输入：训练数据集$D$，停止条件 

> 输出：分类树$f(x)$

> (1)遍历训练数据集，对于每一个特征$A_i$及其可能的取值$a_i$，计算使用$A_i=a_i$作为切分点将训练集分为$D_1,D_2$情况下数据集的基尼指数
> (2)从全部可能的特征$A_i$和其对应的可能取值$a_i$中，选择基尼指数最小的特征和取值来切分当前数据集为$D_1,D_2$
> (3)对得到的两个子集递归的调用（1）（2）步骤，直到满足停止条件
> (4)生成决策树
#### CART的剪枝
CART剪枝主要由两步构成：
- （1）首先依据算法从之前生成步骤产生的决策树$T_0$底端开始不断剪枝，直到$T_0$的根节点，形成一个子树序列$\{T_0,T_1,...,T_n\}$
- （2）通过交叉验证的方式，根据选定的评价函数，从子树序列中选取最优子树。

> 输入：CART算法生成的决策树$T_0$

> 输出：最优决策树$T_\alpha$

> (1)设$k=0,T=T_0,\alpha=+\infty$
> (2)对**内部结点t**自下而上的计算$C(T_t),|T_t|$以及$$g(t)=\frac{C(t)-C(T_t)}{|T_t|-1}$$ $$\alpha =min(\alpha ,g(t))$$这里$T_t$表示以$t$为根节点的子树，$C(T_t)$是该子树对训练数据的预测误差，$|T_t|$是$T_t$的叶节点个数
> (3)对于$g(t)=\alpha$的内部结点$t$进行剪枝，并对叶节点$t$以多数表决的方式决定其类，得到树$T$
> (4)更新$k=k+1,\alpha_{k}=\alpha,T_k=T$
> (5)如果$T_k$不是由根节点集两个叶节点组成的树，则返回步骤（2）；否则令$T_k=T_n$
> (6)采用交叉验证在得到的子树序列$\{T_0,T_1,...,T_n\}$找到最优子树
