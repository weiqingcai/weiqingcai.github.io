---
title: 统计学习方法--K近邻
date: 2019-06-02 16:40:00
tags: [统计学习,机器学习]
categories: 大数据与网络安全
---

## K近邻概述
K近邻算法是一种基本分类与回归模型，该算法假定给定一个实例已经标定的训练数据集，在分类或回归时对新的实例，根据其K个最近邻的训练实例的类别，通过多数表决的方式进行预测，属于判别模型。K值得选择，距离度量，分类决策规则是K近邻算法的三个基本要素。

## K近邻算法

> 输入：训练数据集$T=\{(x_1,y_1),(x_2,y_2)...(x_N,y_N)\};$其中 $x_i \in \chi \subseteq R^n$为实例的特征向量 $y_i \in \gamma=\{ c_1,c_2,...c_K\}$为实例的类别$i=1,2,...N$;实例特征向量$x$

> 输出：实例$x$所属的类$y$

> (1) 根据给定的距离度量，在给定训练集$T$中找到与$x$最近邻的K个点，涵盖这K个点的$x$的邻域记做$N_k(x)$
> (2) 在$N_k(x)$中根据分类决策规则（如多数表决）决定$x$的类别$y$
>$$y=\arg max_{c_j}\sum_{x_i\in N_k(x)}I(y_i=c_j),i=1,2,...,N;j=1,2,3,...,K$$其中$I$为指示函数，即当$y_i=c_j$时$I$为$1$，否则$I$为$0$

> 注: K近邻模型的特殊情况是当K=1时，即对于输入实例，选取最训练集中与其最近的点作为输入实例的类别。

### 距离度量

在K近邻算法中需要通过距离这一度量单位来评价两个实例点之间的距离，如何选取合适的距离度量方式依据于具体的应用背景。

K近邻中关于距离的一般定义为:设特征空间$\chi$是$n$维实数向量空间$R^n,x_i,x_j\in \chi,x_i=(x_i^{(1)},x_i^{(2)},...,x_i^{(n)})^T,x_j=(x_j^{(1)},x_j^{(2)},...,x_j^{(n)})^T,x_i,x_j$的$L_p$距离定义为：$$L_p(x_i,x_j)=(\sum_{l=1}^{n}|x_i^{(l)}-x_j^{(l)}|^p)^{\frac{1}{p}},p\ge 1$$

当$p=2$时，称为欧氏距离，即$L_2(x_i,x_j)=(\sum_{l=1}^{n}|x_i^{(l)}-x_j^{(l)}|^2)^{\frac{1}{2}}$

当$p=1$时，称为曼哈顿距离，即$L_1(x_i,x_j)=\sum_{l=1}^{n}|x_i^{(l)}-x_j^{(l)}|$

当$p=\infty$时，$L_p$为各个坐标距离的最大值$L_\infty(x_i,x_j)=\max_l|x_i^{(l)}-x_j^{(l)}|$

### K值的选择

如果选取较小的K值，相当于用较小的邻域中的训练实例进行预测，预测结果会对近邻的实例点非常敏感。即K值得减小会使整体模型变得复杂，容易发生过拟合。

如果选取较大的K值，相当于用较大的领域中的训练实例进行预测，这时与输入实例较远的训练实例也会对预测产生影响。即K值得增大会使整体模型变得简单。

在应用中，K值一般选取一个较小的数值，通常采用交叉验证发来选取最优的K值。

### 分类决策规则

#### 多数表决规则
K近邻法中常采用的是分类决策规则是多数表决，即由输入实例的K个近邻的训练实例中的多数类决定输入实例的类别。多数表决规则有如下解释：如果分类的损失函数为0-1算是函数，分类函数为$$f:R^n\to\{c_1,c_2,...,c_K\}$$,那么误分类的概率就是$$P(Y\ne f(X))=1-P(Y=f(X))$$,对于给定的输入实例$x\in\chi$,其最近邻的K个训练实例点构成集合$N_k(x)$如果涵盖$N_k(x)$的区域的类别是$c_j$那么误分类率是$$\frac{1}{k}\sum_{x_i\in N_k(x)}I(y_i\ne c_j)=1-\frac{1}{k}\sum_{x_i\in N_k(x)}I(y_i=c_j)$$由公式可知，要使误分类率（经验风险）最小，就要使$\sum_{x_i\in N_k(x)}I(y_i=c_j)$最大，即就要选取$N_k(x)$集合中实例最多的类作为输入实例的类别。