---
title: 统计学习方法--提升方法
date: 2019-08-1 11:10:37
tags: [统计学习,机器学习]
categories: 大数据与网络安全
---

## 提升方法概述
提升方法基于这样一种思想：对于任何一个复杂任务而言，将多个专家的判断进行合适的综合得到的判断，要比任何单独一位专家做出的单独判断要好。基于这种思想，提升方法在分类问题中通过改变训练样本权重，学习多个分类器，并将这些分类器合理组合来提高性能。这样对于提升方法来说有两个主要的问题需要解决：（1）如何在每一轮中改变训练数据的权值或者概率分布（2）如何将弱分类器组合成一个强分类器。

## AdaBoost算法
AdaBoost算法是代表性的提升算法。针对提升方法中的两个主要问题，AdaBoost采用的解决办法是：（1）对于第一个问题，AdaBoost提高前一轮弱分类器错误分类样本的权值，而降低那些被正确分类样本的权值。这样一来，前一轮中错误分类样本在后一轮的训练中能得到更大的关注。（2）对于第二个问题，AdaBoost采用加权多数表决的方式来组合各个弱分类器。即加大分类误差率小的分类器的投票权重，使其在表决中起较大作用，减小分类误差率大的分类器的投票权重，使其在表决中其较小作用。

> 输入：训练数据集$T=\{(x_1,y_1),(x_2,y_2)...(x_N,y_N)\}$，其中$x_i\in\chi\subseteq R^n,y_i\in Y=\{-1,+1\}$;弱学习算法

> 输出：最终分类器$G(x)$

> (1) 初始化训练数据的权值分布$$D_1 = (\omega_{11},...,\omega_{1i},...,\omega_{1N}), \omega_{1i}=\frac{1}{N},i=1,2,...N$$
> (2) 对$m=1,2,...,M$
>   - (a) 使用具有权值分布$D_m$的训练数据学习，得到基本分类器$G_m(x):\chi\rightarrow\{-1,+1\}$
>   - (b) 计算$G_m(x)$在训练数据集上的分类误差率$$e_m=\sum_{i=1}^{N}P(G_m(x_i)\not ={y_i})=\sum_{i=1}^{N}\omega_{mi}I(G_m(x_i)\not ={y_i})=\sum_{G_m(x_i)\not ={y_i}}\omega_{mi}$$其中$\omega_{mi}$表示第$m$轮中第$i$个实例的权值
>   - (c) 计算$G_m(x)$的系数$$\alpha_m=\frac{1}{2}\log\frac{1-e_m}{e_m}$$
>   - (d) 更新训练数据的权值分布$$D_{m+1}=(\omega_{(m+1,1)},...,\omega_{(m+1,i)},...,\omega_{(m+1,N)})$$ $$\omega_{(m+1,i)}=\frac{\omega_{mi}}{Z_m}\exp(-\alpha_m y_i G_m(x_i)),i=1,2,...,N;$$ $Z_m$为规范化因子 $$Z_m=\sum_{i=1}^{N}\omega_{mi}\exp(-\alpha_m y_i G_m(x_i)$$
> (3) 构建基本分类器的线性组合$$f(x)=\sum_{m=1}^{M}\alpha_m G_m(x)$$得到最终的分类器$$G(x)=sign(f(x))=sign(\sum_{m=1}^{M}\alpha_m G_m(x))$$
