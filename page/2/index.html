<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  

  
  <title>程序小栈</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="这是我的小栈，记录点滴">
<meta property="og:type" content="website">
<meta property="og:title" content="程序小栈">
<meta property="og:url" content="http://yoursite.com/page/2/index.html">
<meta property="og:site_name" content="程序小栈">
<meta property="og:description" content="这是我的小栈，记录点滴">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="WeiQingcai">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="程序小栈" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  
<link rel="stylesheet" href="/css/style.css">

<meta name="generator" content="Hexo 5.4.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">程序小栈</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="搜索"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main">
  
    <article id="post-《统计学习方法》笔记--支持向量机" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/07/12/%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E3%80%8B%E7%AC%94%E8%AE%B0--%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/" class="article-date">
  <time datetime="2019-07-12T08:49:37.000Z" itemprop="datePublished">2019-07-12</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B8%8E%E7%BD%91%E7%BB%9C%E5%AE%89%E5%85%A8/">大数据与网络安全</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/07/12/%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E3%80%8B%E7%AC%94%E8%AE%B0--%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/">统计学习方法--支持向量机</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="支持向量机概述"><a href="#支持向量机概述" class="headerlink" title="支持向量机概述"></a>支持向量机概述</h2><p>支持向量机是一种二类分类模型，它的基本模型是定义在特征空间上的间隔最大的线性分类器。支持向量机由简至繁依次可分为：线性可分支持向量机，线性支持向量机，非线性支持向量机。当数据线性可分时，通过硬间隔最大化的约束来学习一个分类器，称为线性可分支持向量机；当数据近似线性可分时，通过软间隔最大化的约束来学习一个分类器，称为线性支持向量机；当数据线性不可分时通过使用核技巧及软间隔最大化，学习非线性支持向量机。故此，支持向量机可以简化的描述为通过训练数据集来寻找一个能够将数据正确分类的超平面，此分离超平面满足数据集中的数据点距此超平面的间隔（软间隔/硬间隔）最大化。</p>
<h2 id="基础前提"><a href="#基础前提" class="headerlink" title="基础前提"></a>基础前提</h2><h3 id="函数间隔和几何间隔"><a href="#函数间隔和几何间隔" class="headerlink" title="函数间隔和几何间隔"></a>函数间隔和几何间隔</h3><p>间隔最大化是贯穿于整个支持向量机模型的最核心的部分。一般来说，一个点距离分离超平面的远近可以表示分类预测的确信程度，在分离超平面确定的情况下，若一个点离超平面越远则分类正确的可信度越高。故此，支持向量机寻求间隔最大化的意义在于分离超平面不仅可以正确分类数据点，而且对于最难分的实例（距离超平面最近的点）也有较大的分类可信度，因此该模型再对未知数据分类时应该也具有较好的性能。</p>
<h4 id="函数间隔"><a href="#函数间隔" class="headerlink" title="函数间隔"></a>函数间隔</h4><blockquote>
<p>对于给定的训练数据集$T$和超平面$(\omega,b)$，</p>
<p>定义超平面$(\omega,b)$关于样本点$(x<em>i,y_i)$的函数间隔为<script type="math/tex">\hat{\gamma_i}=y_i(\omega\cdot x_i+b)</script><br>定义超平面$(\omega,b)$关于训练数据集$T$的函数间隔为超平面$(\omega,b)$关于$T$中所有样本点$(x_i,y_i)$的函数间隔最小值，即$$\hat{\gamma}=\min</em>{i=1,2,..,N}\hat{\gamma_i}$$</p>
</blockquote>
<h4 id="几何间隔"><a href="#几何间隔" class="headerlink" title="几何间隔"></a>几何间隔</h4><p>由于函数间隔在当$\omega$和$b$成比例的改变时候，得到的分离超平面并没有变化，但是函数间隔却也成比例的改变，故此自然想到应该对其进行规范化处理，令$||\omega||=1$，这样就得到了几何间隔的定义。</p>
<blockquote>
<p>对于给定的训练数据集$T$和超平面$(\omega,b)$</p>
<p>定义超平面$(\omega,b)$关于样本点$(x<em>i,y</em>)i$的几何间隔为<script type="math/tex">\gamma_i=y_i(\frac{\omega}{||\omega||}\cdot x_i+\frac{b}{||\omega||})</script><br>定义超平面$(\omega,b)$关于训练数据集$T$的几何间隔为超平面$(\omega,b)$关于$T$中所有样本点$(x<em>i,y_i)$的几何间隔最小值，即$$\gamma=\min</em>{1,2,…,N}\gamma_i$$</p>
</blockquote>
<h2 id="线性可分支持向量机与硬间隔最大化"><a href="#线性可分支持向量机与硬间隔最大化" class="headerlink" title="线性可分支持向量机与硬间隔最大化"></a>线性可分支持向量机与硬间隔最大化</h2><p>给定线性可分训练数据集，通过间隔最大化(几何间隔最大化)或等价的求解相应的凸二次规划问题得到的分离超平面为$\omega^<em>\cdot x+b^</em>=0$,相应的分类决策函数为$f(x)=sign(\omega^<em>\cdot x+b^</em>)$。线性可分支持向量机可以表述为下列形式：<script type="math/tex">\max_{\omega,b}\gamma</script> <script type="math/tex">s.t. y_i(\frac{\omega}{||\omega||}\cdot x_i+\frac{b}{||\omega||})\geq\gamma,i=1,2...,N</script>即转化为了一个约束最优化问题。依据函数间隔和几何间隔的关系及约束最优化的求解方式，可以将上述求解最大值的问题转化为求解最小值的问题，即：<script type="math/tex">\min_{\omega,b}\frac{1}{2}||\omega||^2</script> <script type="math/tex">s.t. y_i(\omega\cdot x_i+b)-1\geq 1,i=1,2...,N</script></p>
<p>故此可以有线性可分支持向量机算法为：</p>
<blockquote>
<p>输入：线性可分训练数据集$T={(x_1,y_2),(x_2,y_2),…,(x_N,y_N)}$,其中$x_i\in X=R^n,y_i\in Y={+1,-1},i=1,2,…,N$</p>
<p>输出：最大间隔分离超平面及分类决策函数</p>
<p>(1) 构造并求解约束最优化问题：<script type="math/tex">\min_{\omega,b}\frac{1}{2}||\omega||^2</script> <script type="math/tex">s.t. y_i(\omega\cdot x_i+b)-1\geq 1,i=1,2...,N</script>得到最优解$\omega^<em>$,$b^</em>$<br>(2) 由此得到分离超平面：<script type="math/tex">\omega^* \cdot x+b^* = 0</script>,分类决策函数<script type="math/tex">f(x)=sign(\omega^* \cdot x+b^*)</script></p>
</blockquote>
<h2 id="线性支持向量机与软间隔最大化"><a href="#线性支持向量机与软间隔最大化" class="headerlink" title="线性支持向量机与软间隔最大化"></a>线性支持向量机与软间隔最大化</h2><p>当数据集线性不可分时表明对于数据集中的一些实例，上述约束方法中的约束不等式不能都成立，即存在一些实例，无法满足函数间隔大于等于1的约束要求。为了解决这个问题，可以对每个样本点引入一个松弛变量$\zeta<em>i\geq0$,使得函数间隔加上松弛变量大于等于1，即约束条件放宽为:$y_i(\omega\cdot x_i+b)\geq 1-\zeta_i$,同时对每个松弛变量支付一个代价$\zeta_i$。目标函数则变为$\frac{1}{2}||\omega||^2+C \sum</em>{i=1}^{N}\zeta<em>i$，这里$C&gt;0$称为惩罚参数，$C$值大时对误分类的惩罚增大，$C$值小时对误分类的惩罚小。故由此可得到线性支持向量机算法的原始表达形式为：$$\min</em>{\omega,b,\zeta} \frac{1}{2}||\omega||^2+C\sum_{i=1}^N\zeta_i<script type="math/tex">$$s.t. y_i(\omega\cdot x_i+b)\geq 1-\zeta_i,i=1,2,...,N</script> <script type="math/tex">\zeta_i\geq 0,i=1,2,...,N</script>这样根据原始问题求解得到$\omega^<em>$,$b^</em>$即可得到分离超平面以及分类决策函数</p>
<h2 id="非线性支持向量机与核函数"><a href="#非线性支持向量机与核函数" class="headerlink" title="非线性支持向量机与核函数"></a>非线性支持向量机与核函数</h2><p>对于线性分类问题，线性支持向量机是一种非常有效的方法。但是有时要解决的是非线性问题，此时就需要收盘每个非线性支持向量机，其重点在于利用核技巧。核技巧不仅应用于支持向量机，而且也可以应用于其他统计学问题中。核技巧就是通过首先使用一个变换将原空间的数据映射到新的空间，然后在新空间里用线性分类学习方法从训练数据中学习分类模型。</p>
<p>核技巧的基本想法是通过一个非线性变换将输入空间（欧式空间$R^n$或离散集合）对应于一个特征空间（希尔伯特空间$\mathrm{H}$），使得在输入空间$R^n$中的超曲面模型对应于特征空间$\mathrm{H}$中的超平面模型（支持向量机）。这样，分类问题的学习任务通过在特征空间中求解线性支持向量机就可以完成。</p>
<h3 id="核函数"><a href="#核函数" class="headerlink" title="核函数"></a>核函数</h3><blockquote>
<p>设$\chi$是输入空间（欧式空间$R^n$的子集或离散集合），又设$\mathrm{H}$为特征空间（希尔伯特空间），如果存在一个从$\chi$到$\mathrm{H}$的映射<script type="math/tex">\phi(x):\chi \to \mathrm{H}</script>使得对所有$x,z\in\chi$,函数$K(x,z)$满足条件<script type="math/tex">K(x,z)=\phi(x)\cdot\phi(z)</script>则称$K(x,z)$为核函数，$\phi(x)$为映射函数，式中$\phi(x)\cdot\phi(z)$为$\phi(x)$和$\phi(z)$的內积</p>
</blockquote>
<h3 id="非线性支持向量机"><a href="#非线性支持向量机" class="headerlink" title="非线性支持向量机"></a>非线性支持向量机</h3><p>从非线性分类训练集，通过核函数与软间隔最大化学习得到的分类决策函数<script type="math/tex">f(x)=sign(\sum_{i=1}^N\alpha_i^*y_iK(x,x_i)+b^ *)</script>称为非线性支持向量机，$K(x,z)$是正定核函数</p>
<blockquote>
<p>输入：训练数据集$T={(x_1,y_1),(x_2,y_2),…,(x_N,y_N)}$其中$x_i\in\chi=R^n,y_i\in Y={-1,+1},i=1,2,…,N$</p>
<p>输出：分类决策函数</p>
<p>(1) 选取适当的核函数$K(x,z)$和适当的参数$C$，构造并求解最优化问题<script type="math/tex">\min_\alpha \frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_j y_iy_jK(x_i,x_j)-\sum_{i=1}^N\alpha_i</script> <script type="math/tex">s.t. \sum_{i=1}^N\alpha_iy_i=0</script> <script type="math/tex">0\leq\alpha_i\leq C,i=1,2,...,N</script>求出最优解$\alpha^<em>=(\alpha_1^</em>,\alpha<em>2^<em>,…,\alpha_N^</em>)^T$<br>(2) 选择$\alpha^<em>$的一个正分量$0&lt;\alpha_j^</em>&lt;C$，计算$$b^*=y_j-\sum</em>{i=1}^N\alpha<em>i^ *y_iK(x_i,x_j)<script type="math/tex">(3) 构造决策函数：</script>f(x)=sign(\sum</em>{i=1}^N\alpha_i^<em>y_iK(x,x_i)+b^ </em>)$$</p>
</blockquote>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2019/07/12/%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E3%80%8B%E7%AC%94%E8%AE%B0--%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/" data-id="ckm1rujxv001688ts21iq1y2d" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag">机器学习</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0/" rel="tag">统计学习</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-《统计学习方法》笔记--逻辑斯谛回归与最大熵模型" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/07/12/%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E3%80%8B%E7%AC%94%E8%AE%B0--%E9%80%BB%E8%BE%91%E6%96%AF%E8%B0%9B%E5%9B%9E%E5%BD%92%E4%B8%8E%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B/" class="article-date">
  <time datetime="2019-07-12T08:49:37.000Z" itemprop="datePublished">2019-07-12</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B8%8E%E7%BD%91%E7%BB%9C%E5%AE%89%E5%85%A8/">大数据与网络安全</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/07/12/%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E3%80%8B%E7%AC%94%E8%AE%B0--%E9%80%BB%E8%BE%91%E6%96%AF%E8%B0%9B%E5%9B%9E%E5%BD%92%E4%B8%8E%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B/">统计学习方法--逻辑斯谛回归</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="逻辑斯谛回归概述"><a href="#逻辑斯谛回归概述" class="headerlink" title="逻辑斯谛回归概述"></a>逻辑斯谛回归概述</h2><p>逻辑斯谛回归的主要思想是：根据现有数据对分类边界线建立回归公式，以此进行分类。这里的“回归”一词源于最佳拟合，表示要找到最佳拟合参数。而最佳拟合参数就是在训练分类器时，通过最优化算法获得。</p>
<h3 id="逻辑斯谛分布"><a href="#逻辑斯谛分布" class="headerlink" title="逻辑斯谛分布"></a>逻辑斯谛分布</h3><blockquote>
<p>设$X$是连续随机变量，$X$服从逻辑斯谛分布是指$X$具有以下分布函数和概率密度函数：<script type="math/tex">F(x)=P(X\leq x)=\frac{1}{1+e^{-(x-\mu)/\gamma}}</script> <script type="math/tex">f(x)=F'(x)=\frac{e^{-(x-\mu)/\gamma}}{\gamma(1+e^{-(x-\mu)/\gamma})^2}</script>其中$\mu$为位置参数，$\gamma \gt 0$为形状参数</p>
</blockquote>
<h3 id="二项式逻辑斯谛回归模型"><a href="#二项式逻辑斯谛回归模型" class="headerlink" title="二项式逻辑斯谛回归模型"></a>二项式逻辑斯谛回归模型</h3><blockquote>
<script type="math/tex; mode=display">P(Y=1|x) = \frac{exp(\omega\cdot x+b)}{1+exp(\omega\cdot x+b)}$$$$P(Y=0|x) = \frac{1}{1+exp(\omega\cdot x+b)}</script><p>这里$x\in R^n$是输入，$Y\in{0,1}$是输出，$\omega\in R^n$和$b\in R^n$是参数，$\omega$称为权值向量，$b$称为偏置，$\omega\cdot x$是$\omega$和$X$的內积</p>
<p>对于给定的输入实例$X$，按照上式可以求得实例属于两种类别的概率，逻辑斯谛回归模型比较两个概率的大小，将实例归入概率较大的那一类中。</p>
</blockquote>
<h3 id="模型参数估计"><a href="#模型参数估计" class="headerlink" title="模型参数估计"></a>模型参数估计</h3><blockquote>
<p>对于给定的训练数据集$T={(x_1,y_1),(x_2,y_2)…(x_N,y_N)}$其中$x_i\in R^n,y_i\in {0,1}$可以应用<code>极大似然估计</code>来得到模型的参数，从而得到逻辑斯谛回归模型。</p>
<p>设<script type="math/tex">P(Y=1|x)=\pi(x),P(Y=0|x)=1-\pi(x)</script>则可得似然函数<script type="math/tex">\prod_{i=1}^N[\pi(x_i)]^{y_i}[1-\pi(x_i)]^{1-y_i}</script>对数似然函数为：<script type="math/tex">L(\omega)=\sum_{i=1}^N[y_i\log\pi(x)+(1-y_i\log(1-\pi(x_i)))]</script>即最终变成求解$L(\omega)$极大值的问题。一般可以采用<code>梯度下降</code>或者是<code>拟牛顿法</code>求解</p>
<p>最后求出$\omega$的极大似然估计值即可得到逻辑斯谛回归模型。</p>
</blockquote>
<h3 id="多项式逻辑斯谛回归"><a href="#多项式逻辑斯谛回归" class="headerlink" title="多项式逻辑斯谛回归"></a>多项式逻辑斯谛回归</h3><p>二项式逻辑斯谛回归应用于二类分类问题，将其推广到多项式逻辑斯谛回归就可以应用于多类分类问题。</p>
<blockquote>
<p>假设离散型随机变量$Y$的取值集合是${1,2,…,K}$则多项式逻辑斯谛回归模型是<script type="math/tex">P(Y=k|x)=\frac{\exp(\omega_k\cdot x)}{1+\sum_{k=1}^{K-1}\exp(\omega_k \cdot x)},  k=1,2,3...,K-1</script> <script type="math/tex">P(Y=K|x)=\frac{1}{1+\sum_{k=1}^{K-1}\exp(\omega_k \cdot x)}</script>这里$x\in R^{n+1},\omega_k\in R^{n+1}$这是将偏置项拓展到权值向量$\omega$和输入向量$x$中。</p>
</blockquote>
<h3 id="逻辑斯谛回归模型和线性回归模型，SVM模型的异同"><a href="#逻辑斯谛回归模型和线性回归模型，SVM模型的异同" class="headerlink" title="逻辑斯谛回归模型和线性回归模型，SVM模型的异同"></a>逻辑斯谛回归模型和线性回归模型，SVM模型的异同</h3><p>总体而言，三者都属于线性模型，只是通过计算得到的线性平面的用法不同。逻辑斯谛回归和SVM是分类模型，线性回归属于回归模型。</p>
<h4 id="关于三种模型异同的相关文章："><a href="#关于三种模型异同的相关文章：" class="headerlink" title="关于三种模型异同的相关文章："></a>关于三种模型异同的相关文章：</h4><ul>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_34032792/article/details/87134490">线性模型（线性回归、感知机和逻辑斯谛回归）</a></li>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/sunflower_sara/article/details/81713449">SVM简介、SVM与感知机、逻辑回归LR的区别</a></li>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/zrh_CSDN/article/details/80920329">逻辑斯蒂回归和感知机模型、支持向量机模型对比</a></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2019/07/12/%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E3%80%8B%E7%AC%94%E8%AE%B0--%E9%80%BB%E8%BE%91%E6%96%AF%E8%B0%9B%E5%9B%9E%E5%BD%92%E4%B8%8E%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B/" data-id="ckm1rujxy001f88tsfhkdhqxg" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag">机器学习</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0/" rel="tag">统计学习</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-《统计学习方法》笔记--决策树" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/06/16/%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E3%80%8B%E7%AC%94%E8%AE%B0--%E5%86%B3%E7%AD%96%E6%A0%91/" class="article-date">
  <time datetime="2019-06-16T08:49:37.000Z" itemprop="datePublished">2019-06-16</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B8%8E%E7%BD%91%E7%BB%9C%E5%AE%89%E5%85%A8/">大数据与网络安全</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/06/16/%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E3%80%8B%E7%AC%94%E8%AE%B0--%E5%86%B3%E7%AD%96%E6%A0%91/">统计学习方法--决策树</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="决策树概述"><a href="#决策树概述" class="headerlink" title="决策树概述"></a>决策树概述</h2><p>决策树模型呈树形结构，在分类过程中表示基于特征对实例进行分类的过程。决策树模型可以视为if-then规则的集合，也可以视为是定义在特征空间与类别空间上的条件改了分布。主要优点是模型具有很好的可解释性，分类速度快，缺点是构建决策树时用的特征序列对分类效果有较大的影响。决策树学习过程通常包括三个步骤：<strong>特征选择</strong>，<strong>决策树的生成</strong>，<strong>决策树的修剪</strong></p>
<h2 id="决策树模型"><a href="#决策树模型" class="headerlink" title="决策树模型"></a>决策树模型</h2><p>分类决策树模型是一种描述对实例进行分类的树形结构。决策树由节点和有向边组成。节点包括两种类型：内部节点和叶节点。内部节点表示一个特征或属性，叶节点表示一个类。</p>
<p>决策树学习的本质是从给定的训练数据集中依据属性或特征归纳出一组分类规则。与给定的训练数据集相符合的分类规则可能有多个，决策树模型就是需要从多个符合的分类规则中找到损失最小的，泛化能力最好的一组分类规则。决策树常用的损失函数是正则化的极大似然函数，决策树的学习策略是以损失函数为目标函数的最小化。由于所有可能的决策树组成的解空间较大，从中找到最优的决策树是NP完全问题，因此一般多采用启发式算法来近似求解。</p>
<p>决策树学习的算法通常是一个递归的选择最优特征的过程。从可选的特征集合中选出最优的特征（即依据该特征能最有效的将训练数据集分类），按照这一特征将数据集分割成子集，该特征作为这些子集的根节点。如果这些子集已经基本可以正确分类，那么构建叶节点，并将这些子集分到所对应的的叶节点中去。如果还有子集不能被正确分类，那么对这些子集选取新的最优特征，继续对其进行分割，直至所有训练数据都被正确分类。至此就构建出来一颗决策树。</p>
<p>通过上述步骤构建的决策树可以对训练数据集进行很好的分类，但是并不一定有很好的泛化能力，即可能发生过拟合现象。为了增强其泛化能力，我们需要对构建好的决策树进行自底向上的剪枝，即将树变得更简单一点。通过去掉决策树中过于细分的叶节点，使其回退到父节点甚至更高节点，用父节点或更高节点作为新的叶节点。</p>
<h3 id="特征选择"><a href="#特征选择" class="headerlink" title="特征选择"></a>特征选择</h3><p>特征选择在于选取对训练数据具有良好分类能力的特征，这样可以提高决策树的学习效率。通常特征选择的准则是信息增益或信息增益比。</p>
<h4 id="熵"><a href="#熵" class="headerlink" title="熵"></a>熵</h4><p>在信息论和概率统计中，熵是表示随机变量不确定性的度量。设$X$是一个取有限个值得离散随机变量，其概率分布为:<script type="math/tex">P(X=x_i)=p_i, i=1,2,3...,n</script>,则随机变量$X$的熵定义为:<script type="math/tex">H(X)=-\sum_{i=1}^np_i\log p_i</script>或(从定义可知和X的取值无关，只和其分布有关)<script type="math/tex">H(p)=-\sum_{i=1}^np_i\log p_i</script>熵有两种单位：（1）当公式中的对数以2为底时，单位为比特(bit)；（2）当公式中的对数以e为底时，单位为纳特(nat)。<strong>熵越大，随机变量的不确定性就越大。</strong></p>
<h4 id="条件熵"><a href="#条件熵" class="headerlink" title="条件熵"></a>条件熵</h4><p>设有随机变量$(X,Y)$其联合概率分布为:<script type="math/tex">P(X=x_i,Y=y_j)=p_{ij},i=1,2,...n,j=1,2,...m</script>条件熵$H(Y|X)$表示在已知随机变量$X$的条件下随机变量$Y$的不确定性，定义为$X$给定条件下$Y$的条件概率分布的熵对$X$的数学期望：<script type="math/tex">H(Y|X)=\sum_{i=1}^np_iH(Y|X=x_i)</script> <script type="math/tex">p_i=P(X=x_i),i=1,2,...,n</script></p>
<h4 id="经验熵和经验条件熵"><a href="#经验熵和经验条件熵" class="headerlink" title="经验熵和经验条件熵"></a>经验熵和经验条件熵</h4><p>当熵和条件熵中的概率由数据估计（特别是极大似然估计）得到时，对应的熵与条件熵分别称为<strong>经验熵</strong>和<strong>经验条件熵</strong>若有概率为0的，则令$0\log0=0$</p>
<h4 id="信息增益（互信息）"><a href="#信息增益（互信息）" class="headerlink" title="信息增益（互信息）"></a>信息增益（互信息）</h4><p>信息增益（也称为互信息）表示得知特征$X$的信息后特征$Y$的信息不确定性减少的程度，反应了特征$X$对于其他特征不确定性的影响程度。</p>
<blockquote>
<p>特征$A$对训练数据集$D$的信息增益$g(D,A)$定义为集合$D$的经验熵$H(D)$与特征$A$在给定条件下$D$的经验条件熵$H(D|A)$之差，即<script type="math/tex">g(D,A)=H(D)-H(D|A)</script></p>
</blockquote>
<p><strong>信息增益大的特征具有更强的分类能力（即表示在已知该特征的情况下，整个集合的不确定降低最多）</strong></p>
<h4 id="信息增益比"><a href="#信息增益比" class="headerlink" title="信息增益比"></a>信息增益比</h4><p>以信息增益作为选择特征的准则，存在偏向于选择取值较多的特征的问题。即当一个特征可能的取值较多时，其计算出来的信息增益可能会较高，但是并不一定就一定是一个更有效的分类特征。采用信息增益比可以对这一问题进行校正，这是特征选择的另一准则。</p>
<blockquote>
<p>特征$A$对训练数据集$D$的信息增益比$gR(D,A)$定义为其信息增益$g(D,A)$与训练数据集$D$关于特征$A$的值的熵$H<em>A(D)$之比，即：<script type="math/tex">gR(D,A)=\frac{g(D,A)}{H_A(D)}</script>,其中,$H_A(D)=-\sum</em>{i=1}^n\frac{|D_i|}{|D|}\log_2\frac{|D_i|}{|D|},n$是特征$A$取值的个数</p>
</blockquote>
<h3 id="决策树生成"><a href="#决策树生成" class="headerlink" title="决策树生成"></a>决策树生成</h3><h4 id="ID3算法-基于信息增益"><a href="#ID3算法-基于信息增益" class="headerlink" title="ID3算法(基于信息增益)"></a>ID3算法(基于信息增益)</h4><p>ID3算法的核心是在决策树各个节点上应用信息增益准则选择特征，递归的构建决策树。</p>
<blockquote>
<p>输入：训练数据集$D$，特征集$A$阈值$\epsilon$</p>
<p>输出：决策树$T$</p>
<p> (1)若$D$中实例属于同一类$C_k$，则$T$为单节点树。并将类$C_k$作为该节点的类标记，返回$T$<br> (2)若$A=\phi$，则$T$为单节点树，并将$D$中实例数最大的类$C_k$作为该节点的类标记，返回$T$<br> (3)否则，按照求解<strong>信息增益</strong>的算法，计算$A$中各特征对$D$的信息增益，选择<strong>信息增益</strong>最大的特征$A_g$<br> (4)如果$A_g$的<strong>信息增益</strong>小于阈值$\epsilon$,则置$T$为单节点树，并将$D$中实例数最大的类$C_k$作为该节点的类标记，返回$T$<br> (5)否则，对$A_g$的每一可能值$a_i$,依$A_g=a_i$将$D$分割为若干非空子集$D_i$，将$D_i$中实例数最大的类作为标记，构建子节点，由节点及其子节点构成树$T$,返回$T$<br> (6)对第$i$个子节点，以$D_i$为训练集，以$A-{A_g}$为特征集，递归调用步骤(1)~(5)，得到子树$T_i$,返回$T_i$</p>
</blockquote>
<h4 id="C4-5算法-基于信息增益比"><a href="#C4-5算法-基于信息增益比" class="headerlink" title="C4.5算法(基于信息增益比)"></a>C4.5算法(基于信息增益比)</h4><p>C4.5算法本质上和ID3算法是一样的，只是采用信息增益比作为特征选择的评价准则。</p>
<blockquote>
<p>输入：训练数据集$D$，特征集$A$阈值$\epsilon$</p>
<p>输出：决策树$T$</p>
<p> (1)若$D$中实例属于同一类$C_k$，则$T$为单节点树。并将类$C_k$作为该节点的类标记，返回$T$<br> (2)若$A=\phi$，则$T$为单节点树，并将$D$中实例数最大的类$C_k$作为该节点的类标记，返回$T$<br> (3)否则，按照求解<strong>信息增益比</strong>的算法，计算$A$中各特征对$D$的信息增益，选择<strong>信息增益比</strong>最大的特征$A_g$<br> (4)如果$A_g$的<strong>信息增益比</strong>小于阈值$\epsilon$,则置$T$为单节点树，并将$D$中实例数最大的类$C_k$作为该节点的类标记，返回$T$<br> (5)否则，对$A_g$的每一可能值$a_i$,依$A_g=a_i$将$D$分割为若干非空子集$D_i$，将$D_i$中实例数最大的类作为标记，构建子节点，由节点及其子节点构成树$T$,返回$T$<br> (6)对第$i$个子节点，以$D_i$为训练集，以$A-{A_g}$为特征集，递归调用步骤(1)~(5)，得到子树$T_i$,返回$T_i$</p>
</blockquote>
<h3 id="决策树的剪枝"><a href="#决策树的剪枝" class="headerlink" title="决策树的剪枝"></a>决策树的剪枝</h3><p>通过前边决策树生成算法的步骤生成的决策树可能对训练数据有很好的拟合效果，但是由于分支过细，可能会包含太多训练集中的信息，导致泛化能力很差，对未知的数据没有准确的分类。解决这个问题的办法是考虑决策树的复杂度，对已生成的决策树进行简化。</p>
<blockquote>
<p>输入：生成算法产生的整个决策树$T$,参数$\alpha$</p>
<p>输出：修剪后的子树$T_\alpha$</p>
<p>(1)计算每个节点的经验熵<br>(2)递归的从树的叶节点向上回缩。设一组叶节点回缩到其父节点之前与之后的整体树分别为$T<em>B,T_A$，其对应的损失函数分别是$C</em>\alpha(T<em>B)$与$C</em>\alpha(T<em>A)$,如果$$C</em>\alpha(T<em>A)\leq C</em>\alpha(T<em>B)$$则进行剪枝，将父节点变为新的叶节点。<br>(3)返回(2)，直至不能继续为止，得到损失函数最小的子树$T</em>\alpha$</p>
<h3 id="CART算法（分类回归树算法）"><a href="#CART算法（分类回归树算法）" class="headerlink" title="CART算法（分类回归树算法）"></a>CART算法（分类回归树算法）</h3><p>分类回归树算法完整的包含了决策树从特征选择，决策树生成，决策树剪枝的完整过程。CART假设决策树为二叉树，通过将内部结点取值按照“是”和“否”来划分数据集，左分支为结点取值为“是”的数据集合，右分支为取值为“否”的结点集合，这样就可以递归的将数据集划分为有限个集合。</p>
</blockquote>
<h4 id="CART的特征选择"><a href="#CART的特征选择" class="headerlink" title="CART的特征选择"></a>CART的特征选择</h4><p>CART的特征选择主要有两种方法：</p>
<ul>
<li>针对回归树，采用平方误差最小化准则来选择特征及特征的切分点；</li>
<li>针对分类树，采用基尼指数作为准则来选择最优特征和最优切分点；<h5 id="基尼指数："><a href="#基尼指数：" class="headerlink" title="基尼指数："></a>基尼指数：</h5><blockquote>
<p>分类问题中，假设有$K$个类，样本点属于第$k$类的概率为$p<em>k$，则概率分布的基尼指数定义为$$Gini(p)=\sum</em>{k=1}^{K}p<em>k(1-p_k)=1-\sum</em>{k=1}^{K}p<em>{k}^2<script type="math/tex">对于给定样本集合$D$，其基尼指数为</script>Gini(p)=1-\sum</em>{k=1}^{K}(\frac{|C_k|}{|D|})^2$$其中$C_k$是$D$中属于第$k$类的样本子集，$K$是类的个数。与熵类似，当基尼指数越大，表明集合的不确定性越大。</p>
</blockquote>
</li>
</ul>
<p>在实际使用中，如果样本集合$D$依据特征$A$的可能取值$a$分割为两个子集$D_1$和$D_2$，则此时样本集合$D$的基尼指数定义为<script type="math/tex">Gini(D,A)=\frac{|D_1|}{|D|}Gini(D_1)+\frac{|D_2|}{|D|}Gini(D_2)</script></p>
<h4 id="CART的生成"><a href="#CART的生成" class="headerlink" title="CART的生成"></a>CART的生成</h4><h5 id="回归树"><a href="#回归树" class="headerlink" title="回归树"></a>回归树</h5><blockquote>
<p>输入：训练数据集$D$，停止计算条件</p>
<p>输出：回归树$f(x)$</p>
<p>(1)遍历寻找最优切分变量（最优切分特征）和切分点（最优切分特征的最优切分取值），使得平方误差损失最小，即求解<script type="math/tex">\min_{j,s}[\min_{c_1}\sum_{x_i \in R_1(j,s)}(y_i-c_1)^2+\min_{c_2}\sum_{x_i\in R_2(j,s)}(y_i-c_2)^2]</script>遍历变量$j$，对固定的切分变量$j$（特征）扫描切分点$s$（特征的取值），选取使得平方误差最小的对$(j,s)$。<br>注：直观的解释就是遍历训练数据集$D$的全部特征，对于每一个特征$A<em>i$的可能取值$a_i$，计算用特征$A_i$取值为$a_i$时切分数据集后，使整个数据集的平方误差最小的特征以及特征对应的取值$（A_i，a_i）$，也就是公式中的$(j,s)$。在找到使得数据集$D$平方误差最小的特征和特征值后，用该特征和特征值切分数据集。<br>(2)使用选定的对$(j,s)$划分区域并计算相应的输出值<script type="math/tex">R_1(j,s)=\{x|x^{(j)}\leq s\},R_2(j,s)=\{x|x^{j}>s\}</script> $$\hat{c_m}=\frac{1}{N_m}\sum</em>{x<em>i\in R_m(j,s)}y_i,x</em>\in R<em>m,m=1,2<script type="math/tex">(3)继续递归的对两个子区域调用步骤（1）和（2），直到满足停止条件
(4)将输入数据集划分为M个区域$R_1,R_2...,R_M$,生成决策树：</script>f(x)=\sum</em>{m=1}^{M}\hat{c_m}I(x\in R_m)$$</p>
</blockquote>
<h4 id="分类树"><a href="#分类树" class="headerlink" title="分类树"></a>分类树</h4><blockquote>
<p>输入：训练数据集$D$，停止条件 </p>
<p>输出：分类树$f(x)$</p>
<p>(1)遍历训练数据集，对于每一个特征$A_i$及其可能的取值$a_i$，计算使用$A_i=a_i$作为切分点将训练集分为$D_1,D_2$情况下数据集的基尼指数<br>(2)从全部可能的特征$A_i$和其对应的可能取值$a_i$中，选择基尼指数最小的特征和取值来切分当前数据集为$D_1,D_2$<br>(3)对得到的两个子集递归的调用（1）（2）步骤，直到满足停止条件<br>(4)生成决策树</p>
<h4 id="CART的剪枝"><a href="#CART的剪枝" class="headerlink" title="CART的剪枝"></a>CART的剪枝</h4><p>CART剪枝主要由两步构成：</p>
<ul>
<li>（1）首先依据算法从之前生成步骤产生的决策树$T_0$底端开始不断剪枝，直到$T_0$的根节点，形成一个子树序列${T_0,T_1,…,T_n}$</li>
<li>（2）通过交叉验证的方式，根据选定的评价函数，从子树序列中选取最优子树。</li>
</ul>
<p>输入：CART算法生成的决策树$T_0$</p>
<p>输出：最优决策树$T_\alpha$</p>
<p>(1)设$k=0,T=T<em>0,\alpha=+\infty$<br>(2)对<strong>内部结点t</strong>自下而上的计算$C(T_t),|T_t|$以及<script type="math/tex">g(t)=\frac{C(t)-C(T_t)}{|T_t|-1}</script> <script type="math/tex">\alpha =min(\alpha ,g(t))</script>这里$T_t$表示以$t$为根节点的子树，$C(T_t)$是该子树对训练数据的预测误差，$|T_t|$是$T_t$的叶节点个数<br>(3)对于$g(t)=\alpha$的内部结点$t$进行剪枝，并对叶节点$t$以多数表决的方式决定其类，得到树$T$<br>(4)更新$k=k+1,\alpha</em>{k}=\alpha,T_k=T$<br>(5)如果$T_k$不是由根节点集两个叶节点组成的树，则返回步骤（2）；否则令$T_k=T_n$<br>(6)采用交叉验证在得到的子树序列${T_0,T_1,…,T_n}$找到最优子树</p>
</blockquote>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2019/06/16/%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E3%80%8B%E7%AC%94%E8%AE%B0--%E5%86%B3%E7%AD%96%E6%A0%91/" data-id="ckm1rujxr000u88tshy3a752r" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag">机器学习</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0/" rel="tag">统计学习</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-《统计学习方法》笔记--朴素贝叶斯" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/06/09/%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E3%80%8B%E7%AC%94%E8%AE%B0--%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF/" class="article-date">
  <time datetime="2019-06-09T08:49:37.000Z" itemprop="datePublished">2019-06-09</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B8%8E%E7%BD%91%E7%BB%9C%E5%AE%89%E5%85%A8/">大数据与网络安全</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/06/09/%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E3%80%8B%E7%AC%94%E8%AE%B0--%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF/">统计学习方法--朴素贝叶斯</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="朴素贝叶斯概述"><a href="#朴素贝叶斯概述" class="headerlink" title="朴素贝叶斯概述"></a>朴素贝叶斯概述</h2><p>朴素贝叶斯法是基于贝叶斯定理与特征条件独立假设的分类方法。通过给定的训练数据集，首先基于特征条件独立的假设学习输入输出的联合概率分布，然后基于此模型，对于给定的$x$，利用贝叶斯定理求出后验概率最大的输出$y$</p>
<h2 id="朴素贝叶斯法"><a href="#朴素贝叶斯法" class="headerlink" title="朴素贝叶斯法"></a>朴素贝叶斯法</h2><blockquote>
<p>输入：训练数据$T={(x<em>1,y_1),(x_2,y_2)…(x_N,y_N)};$其中$x_i=(x_i^{(1)},x_i^{2)},…,x_i^{(n)})^T;$$x_i{(j)}是第i个样本的第j个特征，x_i{(j)}\in {a</em>{j1},a<em>{j2},…,a</em>{jS<em>j}},a</em>{jl}是第j个特征可能取得第l个值，j=1,2,3…,n;l=1,2,…S_j;y_i\in{c_1,c_2,…,c_K};实例x$</p>
<p>输出：实例$x$的分类</p>
<p>(1) 根据给出的训练集计算先验概率和条件概率<script type="math/tex">P(Y=c_k)=\frac{\sum_{i=1}^{N}I(y_i=c_k)}{N},k=1,2,...,K----在给定训练集中c_k类的概率</script> <script type="math/tex">P(X^{(j)}=a_{jl}|Y=c_k)=\frac{\sum_{i=1}^{N}I(x_i^{(j)}=a_{jl},y_i=c_k)}{\sum_{i=1}^NI(y_i=c_k)}----在给定训练集中，已知为c_k类时，第j个特征取值为a_{jl}的概率</script> <script type="math/tex">j=1,2,...,n;l=1,2,...,S_j;k=1,2...,K</script><br>(2) 对于给定实例$x=(x^{(1)},x^{(2)},…,x^{(n)})^T,$计算<script type="math/tex">P(Y=c_k)\prod_{j=1}^nP(X^{(j)}=x^{(j)}|Y=c_k),k=1,2,...,K----对于给定的实例，计算在不同类别下其特征序列(x^{(1)},x^{(2)},...,x^{(n)})出现的概率</script><br>(3)确定实例$x$所属的类<script type="math/tex">y=\arg\max_{c_k}P(Y=c_k)\prod_{j=1}^nP(X^{(j)}=x^{(j)}|Y=c_k)----将实例x归入特征序列出现概率最大的那一类</script></p>
</blockquote>
<h3 id="参数估计"><a href="#参数估计" class="headerlink" title="参数估计"></a>参数估计</h3><p>朴素贝叶斯法主要是通过训练集计算出每个类别$c_k$的先验概率，然后通过贝叶斯公式，计算出相应特征的条件概率，即在已知类别$c_k$的条件下各个特征出现的概率，最后通过求出在给定类别时出现概率最大的类别作为实例的类别。</p>
<h4 id="极大似然估计"><a href="#极大似然估计" class="headerlink" title="极大似然估计"></a>极大似然估计</h4><p>在朴素贝叶斯法中，学习意味着估计$P(Y=c_k)$和$P(X^{j}=x^{(j)}|Y=c_k)$由于前提假设是特征条件相互，因此可以用极大似然估计法来估计相应的概率。</p>
<p>先验概率$P(Y=c<em>k)$的极大似然估计是$$P(Y=c_k)=\frac{\sum</em>{i=1}^{N}I(y_i=c_k)}{N},k=1,2,…,K$$</p>
<p>条件概率$P(X^{(j)}=a<em>{jl}|Y=c_k)$的极大似然估计是$$P(X^{(j)}=a</em>{jl}|Y=c<em>k)=\frac{\sum</em>{i=1}^{N}I(x<em>i^{(j)}=a</em>{jl},y<em>i=c_k)}{\sum</em>{i=1}^NI(y<em>i=c_k)}——在给定训练集中，已知为c_k类时，第j个特征取值为a</em>{jl}的概率<script type="math/tex">$$j=1,2,...,n;l=1,2,...,S_j;k=1,2...,K</script></p>
<h4 id="贝叶斯估计"><a href="#贝叶斯估计" class="headerlink" title="贝叶斯估计"></a>贝叶斯估计</h4><p>考虑到用极大似然估计可能会出现会估计的概率值为0的情况，可以采用贝叶斯估计来代替极大似然估计。具体的，</p>
<p>先验概率的贝叶斯估计为<script type="math/tex">P_\lambda(Y=c_k)=\frac{\sum_{i=1}^{N}I(y_i=c_k)+\lambda}{N+K\lambda}</script></p>
<p>条件概率的贝叶斯估计为<script type="math/tex">P_\lambda(X^{(j)}=a_{jl}|Y=c_k)=\frac{\sum_{i=1}^NI(x_i^{(j)=a_{jl}},y_i=c_k)+\lambda}{\sum_{i=1}^NI(y_i=c_k)+S_j\lambda}</script></p>
<p>上式中$\lambda\geq0$等价于在随机变量各个取值的频数上赋予一个正数$\lambda&gt;0$,当$\lambda=0$时就是极大似然估计。常取$\lambda=1$，这时称为拉普拉斯平滑。显然对于任何的$l=1,2,…S<em>j;k=1,2,…,K$有：$$P</em>\lambda(X^{(j)}=a<em>{jl}|Y=c_k)&gt;0<script type="math/tex"> </script>\sum</em>{l=1}^{S<em>j}P</em>\lambda(X^{(j)}=a_{jl}|Y=c_k)=1$$</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2019/06/09/%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E3%80%8B%E7%AC%94%E8%AE%B0--%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF/" data-id="ckm1rujxw001a88ts0c3921ll" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag">机器学习</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0/" rel="tag">统计学习</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-《统计学习方法》笔记-K近邻" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/06/02/%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E3%80%8B%E7%AC%94%E8%AE%B0-K%E8%BF%91%E9%82%BB/" class="article-date">
  <time datetime="2019-06-02T08:40:00.000Z" itemprop="datePublished">2019-06-02</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B8%8E%E7%BD%91%E7%BB%9C%E5%AE%89%E5%85%A8/">大数据与网络安全</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/06/02/%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E3%80%8B%E7%AC%94%E8%AE%B0-K%E8%BF%91%E9%82%BB/">统计学习方法--K近邻</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="K近邻概述"><a href="#K近邻概述" class="headerlink" title="K近邻概述"></a>K近邻概述</h2><p>K近邻算法是一种基本分类与回归模型，该算法假定给定一个实例已经标定的训练数据集，在分类或回归时对新的实例，根据其K个最近邻的训练实例的类别，通过多数表决的方式进行预测，属于判别模型。K值得选择，距离度量，分类决策规则是K近邻算法的三个基本要素。</p>
<h2 id="K近邻算法"><a href="#K近邻算法" class="headerlink" title="K近邻算法"></a>K近邻算法</h2><blockquote>
<p>输入：训练数据集$T={(x_1,y_1),(x_2,y_2)…(x_N,y_N)};$其中 $x_i \in \chi \subseteq R^n$为实例的特征向量 $y_i \in \gamma={ c_1,c_2,…c_K}$为实例的类别$i=1,2,…N$;实例特征向量$x$</p>
<p>输出：实例$x$所属的类$y$</p>
<p>(1) 根据给定的距离度量，在给定训练集$T$中找到与$x$最近邻的K个点，涵盖这K个点的$x$的邻域记做$N_k(x)$<br>(2) 在$N_k(x)$中根据分类决策规则（如多数表决）决定$x$的类别$y$</p>
<p><script type="math/tex">y=\arg max_{c_j}\sum_{x_i\in N_k(x)}I(y_i=c_j),i=1,2,...,N;j=1,2,3,...,K</script>其中$I$为指示函数，即当$y_i=c_j$时$I$为$1$，否则$I$为$0$</p>
<p>注: K近邻模型的特殊情况是当K=1时，即对于输入实例，选取最训练集中与其最近的点作为输入实例的类别。</p>
</blockquote>
<h3 id="距离度量"><a href="#距离度量" class="headerlink" title="距离度量"></a>距离度量</h3><p>在K近邻算法中需要通过距离这一度量单位来评价两个实例点之间的距离，如何选取合适的距离度量方式依据于具体的应用背景。</p>
<p>K近邻中关于距离的一般定义为:设特征空间$\chi$是$n$维实数向量空间$R^n,x<em>i,x_j\in \chi,x_i=(x_i^{(1)},x_i^{(2)},…,x_i^{(n)})^T,x_j=(x_j^{(1)},x_j^{(2)},…,x_j^{(n)})^T,x_i,x_j$的$L_p$距离定义为：$$L_p(x_i,x_j)=(\sum</em>{l=1}^{n}|x_i^{(l)}-x_j^{(l)}|^p)^{\frac{1}{p}},p\ge 1$$</p>
<p>当$p=2$时，称为欧氏距离，即$L<em>2(x_i,x_j)=(\sum</em>{l=1}^{n}|x_i^{(l)}-x_j^{(l)}|^2)^{\frac{1}{2}}$</p>
<p>当$p=1$时，称为曼哈顿距离，即$L<em>1(x_i,x_j)=\sum</em>{l=1}^{n}|x_i^{(l)}-x_j^{(l)}|$</p>
<p>当$p=\infty$时，$L<em>p$为各个坐标距离的最大值$L</em>\infty(x_i,x_j)=\max_l|x_i^{(l)}-x_j^{(l)}|$</p>
<h3 id="K值的选择"><a href="#K值的选择" class="headerlink" title="K值的选择"></a>K值的选择</h3><p>如果选取较小的K值，相当于用较小的邻域中的训练实例进行预测，预测结果会对近邻的实例点非常敏感。即K值得减小会使整体模型变得复杂，容易发生过拟合。</p>
<p>如果选取较大的K值，相当于用较大的领域中的训练实例进行预测，这时与输入实例较远的训练实例也会对预测产生影响。即K值得增大会使整体模型变得简单。</p>
<p>在应用中，K值一般选取一个较小的数值，通常采用交叉验证发来选取最优的K值。</p>
<h3 id="分类决策规则"><a href="#分类决策规则" class="headerlink" title="分类决策规则"></a>分类决策规则</h3><h4 id="多数表决规则"><a href="#多数表决规则" class="headerlink" title="多数表决规则"></a>多数表决规则</h4><p>K近邻法中常采用的是分类决策规则是多数表决，即由输入实例的K个近邻的训练实例中的多数类决定输入实例的类别。多数表决规则有如下解释：如果分类的损失函数为0-1算是函数，分类函数为<script type="math/tex">f:R^n\to\{c_1,c_2,...,c_K\}</script>,那么误分类的概率就是<script type="math/tex">P(Y\ne f(X))=1-P(Y=f(X))</script>,对于给定的输入实例$x\in\chi$,其最近邻的K个训练实例点构成集合$N<em>k(x)$如果涵盖$N_k(x)$的区域的类别是$c_j$那么误分类率是$$\frac{1}{k}\sum</em>{x<em>i\in N_k(x)}I(y_i\ne c_j)=1-\frac{1}{k}\sum</em>{x<em>i\in N_k(x)}I(y_i=c_j)$$由公式可知，要使误分类率（经验风险）最小，就要使$\sum</em>{x_i\in N_k(x)}I(y_i=c_j)$最大，即就要选取$N_k(x)$集合中实例最多的类作为输入实例的类别。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2019/06/02/%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E3%80%8B%E7%AC%94%E8%AE%B0-K%E8%BF%91%E9%82%BB/" data-id="ckm1rujy0001h88tsd2pldg4x" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag">机器学习</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0/" rel="tag">统计学习</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-《统计学习方法》笔记--感知机" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/05/28/%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E3%80%8B%E7%AC%94%E8%AE%B0--%E6%84%9F%E7%9F%A5%E6%9C%BA/" class="article-date">
  <time datetime="2019-05-28T08:49:37.000Z" itemprop="datePublished">2019-05-28</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B8%8E%E7%BD%91%E7%BB%9C%E5%AE%89%E5%85%A8/">大数据与网络安全</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/05/28/%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E3%80%8B%E7%AC%94%E8%AE%B0--%E6%84%9F%E7%9F%A5%E6%9C%BA/">统计学习方法--感知机</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="感知机概述"><a href="#感知机概述" class="headerlink" title="感知机概述"></a>感知机概述</h2><p>感知机是二类分类的线性模型，输入为实例的特征向量，输出为实例的类别，取+1和-1两个值。感知机本质对应于输入空间的一个超平面，通过将正负两类通过一个超平面划分开来，属于判别式模型。</p>
<h2 id="感知机模型"><a href="#感知机模型" class="headerlink" title="感知机模型"></a>感知机模型</h2><blockquote>
<p>假设输入空间（特征空间）是 $\chi \subseteqq R^n$ 输出空间是 $y={+1,-1}$ 。输入$x\in \chi$表示实例的特征向量，对应于输入空间的点；输出$ y \in Y$表示实例的类别。由输入空间到输出空间的如下函数 <script type="math/tex">f(x)=sign(w \cdot x +b)</script> 称为感知机。其中 $w$ 和$b$称为感知机模型参数， $w\in R^n$ 叫做权值或者权重向量，$b\in R$ 叫做偏置，$w\cdot x$ 表示$w$和$x$的内积，$sign$是符号函数,即<script type="math/tex">sign(x)=\left\{\begin{array}{cc}+1, & x \geqslant0 \\-1, & \ x<0\end{array}\right.</script></p>
</blockquote>
<h2 id="感知机学习策略"><a href="#感知机学习策略" class="headerlink" title="感知机学习策略"></a>感知机学习策略</h2><h3 id="数据集的要求"><a href="#数据集的要求" class="headerlink" title="数据集的要求"></a>数据集的要求</h3><p>运用感知机模型首先要求数据集是线性可分的，即通俗的说就是对于给定的数据集，存在这样一个超平面，能够将数据集的正实例点(y=+1)和负实例点(y=-1)完全正确的划分到超平面的两侧，这样就称为数据集是线性可分的。</p>
<h4 id="形式化定义："><a href="#形式化定义：" class="headerlink" title="形式化定义："></a>形式化定义：</h4><blockquote>
<p>给定一个数据集，<script type="math/tex">T=\{(x_1,y_1),(x_2,y_2)...(x_N,y_N)\}</script>其中 $x_i \in \chi=R^n,y_i \in \gamma={ +1,-1},i=1,2,…N,$如果存在某个超平面S,对所有$y_i=+1$的实例$i$有$w \cdot x_i+b&gt;0$,对所有$y_i=-1$的实例$i$有$w \cdot x_i+b&lt;0$。则称数据集T为线性可分数据集，否则为线性不可分。</p>
</blockquote>
<h3 id="学习策略（损失函数最小化策略）"><a href="#学习策略（损失函数最小化策略）" class="headerlink" title="学习策略（损失函数最小化策略）"></a>学习策略（损失函数最小化策略）</h3><p>感知机$sign(w\cdot x+b)$学习的损失函数定义为：<script type="math/tex">L(w,b)=-\sum_{x_i \in M}y_i(w\cdot x+b)</script>其中$M$为误分类点的集合。</p>
<p>这个损失函数的基本思想就是通过衡量误分类点距离超平面的总距离来计算，实际上计算了感知机器学习的经验风险函数。直观来看，如果没有误分类点，损失函数就是0，而且误分类点越少，离超平面越近。损失函数值就越少。</p>
<blockquote>
<p>注：任意一点到超平面的距离公式为<script type="math/tex">\frac{1}{||w||}|w\cdot x+b|</script>这里$||w||$是$w$的$L_2$范数。在构造感知机学习损失函数时不考虑$\frac{1}{||w||}$</p>
</blockquote>
<h2 id="感知机学习算法"><a href="#感知机学习算法" class="headerlink" title="感知机学习算法"></a>感知机学习算法</h2><p>感知机学习算法有一个直观的理解，当一个实例被误分类时其位于分离超平面错误的一侧，我们通过调整$w，b$的值使得超平面向该误分类点的一侧移动，以减少该误分类点与超平面的距离，直到超平面越过该点，使其被正确分类。</p>
<h3 id="原始形式"><a href="#原始形式" class="headerlink" title="原始形式"></a>原始形式</h3><blockquote>
<p>输入：训练数据集$T={(x_1,y_1),(x_2,y_2)…(x_N,y_N)};$其中 $x_i \in \chi=R^n,y_i \in \gamma={ +1,-1},i=1,2,…N$;学习率$\eta(0&lt;\eta\leqslant1)$</p>
<p>输出：$w,b;$感知机模型$f(x)=sign(w\cdot x+b)$</p>
<p>(1)选取初始值$w_0,b_0$<br>(2)在训练集中选取数据$(x_i,y_i)$<br>(3)如果$y_i(w\cdot x_i+b)\leqslant0$,<script type="math/tex">w\gets w+\eta y_ix_i</script> <script type="math/tex">b\gets b+\eta y_i</script><br>(4)转至(2)，直至训练集中没有误分类点</p>
</blockquote>
<h3 id="对偶形式"><a href="#对偶形式" class="headerlink" title="对偶形式"></a>对偶形式</h3><blockquote>
<p>输入：线性可分数据集$T={(x_1,y_1),(x_2,y_2)…(x_N,y_N)};$其中 $x_i \in \chi=R^n,y_i \in \gamma={ +1,-1},i=1,2,…N$;学习率$\eta(0&lt;\eta\leqslant1)$</p>
<p>输出：$\alpha,b;$感知机模型$f(x)=sign(\sum_{j=i}^{N}\alpha_jy_jx_j\cdot x+b)$,其中$\alpha = (\alpha_1,\alpha_2,…\alpha_N)^T$</p>
<p>(1)$\alpha\gets0,b\gets0$<br>(2)在训练数据集中选取数据$(x<em>i,y_i)$<br>(3)如果$y_i(\sum</em>{j=1}^{N}\alpha_jy_jx_j\cdot x_i+b)\leqslant0$,<script type="math/tex">\alpha_i\gets\alpha_i+\eta</script> <script type="math/tex">b\gets b+\eta y_i</script><br>(4)转至(2)直至没有误分类数据</p>
</blockquote>
<h3 id="联系与区别"><a href="#联系与区别" class="headerlink" title="联系与区别"></a>联系与区别</h3><p>在最开始看到两种形式的感知机学习算法时有点不明白为什么会在原始形式上发展出对偶形式，因为虽然对偶形式的算法可以通过提前计算一些变量来获得一定的加速，但是本质上还是要迭代更新权重来进行训练。</p>
<p>后来明白这是由于从不同的角度去解决问题而给出的算法，对偶通俗理解即从不同角度去解答相似问题，但问题的解是相通的，甚至是一样的。另外感知机的对偶算法在特征维度很高时提升性能的效果很明显，由于可以提前计算出$x_j\cdot y_j$这样每次在判断误分类时可以直接查找表中数据，不用重新计算两者内积。例如对于特征向量维度为N的数据集，可以把每次的$N\cdot N$运算降低到$N$<br>详细可以阅读知乎回答<a target="_blank" rel="noopener" href="https://www.zhihu.com/question/26526858/answer/131591887">如何理解感知机学习算法的对偶形式？</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2019/05/28/%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E3%80%8B%E7%AC%94%E8%AE%B0--%E6%84%9F%E7%9F%A5%E6%9C%BA/" data-id="ckm1rujxs000w88tsem1idlrt" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag">机器学习</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0/" rel="tag">统计学习</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-《统计学习方法》笔记--概述" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/05/28/%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E3%80%8B%E7%AC%94%E8%AE%B0--%E6%A6%82%E8%BF%B0/" class="article-date">
  <time datetime="2019-05-28T08:49:37.000Z" itemprop="datePublished">2019-05-28</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B8%8E%E7%BD%91%E7%BB%9C%E5%AE%89%E5%85%A8/">大数据与网络安全</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/05/28/%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E3%80%8B%E7%AC%94%E8%AE%B0--%E6%A6%82%E8%BF%B0/">统计学习方法--概述</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="统计学习概述"><a href="#统计学习概述" class="headerlink" title="统计学习概述"></a>统计学习概述</h2><p>统计学习是关于计算机基于数据构建概率统计模型并运用模型对数据进行预测与分析的学科。目的是使得计算机系统通过运用数据及统计学习方法提高系统性能。</p>
<p>统计学习方法可以概述如下：</p>
<blockquote>
<p>从给定的，有限的用于训练的数据集合出发，假设数据是独立同分布产生的；并且假设要学习的模型属于某个函数的集合，称为假设空间；应用某个评价准则，从假设模型中选取一个最优模型，使它对已知的训练数据及未知的测试数据在给定的评定准则下有最优的预测；最优模型的选取由算法实现。</p>
</blockquote>
<h2 id="统计学习分类"><a href="#统计学习分类" class="headerlink" title="统计学习分类"></a>统计学习分类</h2><h3 id="基本分类"><a href="#基本分类" class="headerlink" title="基本分类"></a>基本分类</h3><ol>
<li><p>监督学习：从标注数据中学习预测模型的机器学习问题，本质是学习输入到输出的映射统计规律</p>
</li>
<li><p>无监督学习：从无标注的数据中学习预测模型的机器学习问题，本质是学习数据中的统计规律或潜在结构</p>
</li>
<li><p>强化学习：智能系统在与环境的连续交互中学习最后最优行为策略的机器学习问题，本质是学习最优的序贯决策。智能系统的目的不是短期奖励的最大化，二十长期累积奖励的最大化。</p>
</li>
<li><p>半监督学习：指利用标注数据和未标注数据学习预测模型的机器学习问题。主要是通过未标注的数据中的信息，辅助标注数据进行监督学习。</p>
</li>
<li><p>主动学习：指机器不断主动给出具有较高区分度的实例进行标注，然后利用标注数据学习预测模型的机器学习问题。</p>
</li>
</ol>
<h3 id="按照模型分类"><a href="#按照模型分类" class="headerlink" title="按照模型分类"></a>按照模型分类</h3><ol>
<li>概率模型/非概率模型：两者的区别不在于输入和输出之间的映射，而在于模型的内在结构。概率模型一定可以表示为联合概率分布的形式，而非概率模型则不一定存在这样的联合概率分布。</li>
<li>线性模型/非线性模型：根据模型的函数表达形式是否是线性函数分为线性模型和非线性模型。</li>
<li>参数化模型/非参数化模型：参数化模型假设模型参数的维度固定，模型可以由有限维参数完全刻画，非参数化模型假设模型参数的维度不固定，随着训练数据量的增加而不断增大。</li>
</ol>
<h3 id="按照算法分类"><a href="#按照算法分类" class="headerlink" title="按照算法分类"></a>按照算法分类</h3><ol>
<li>在线学习：每次接受一个样本，进行预测，之后学习模型，并不断重复该循环步骤的机器学习问题。</li>
<li>批量学习：一次接受全部的数据，学习模型之后进行预测。</li>
</ol>
<h3 id="按照技巧分类"><a href="#按照技巧分类" class="headerlink" title="按照技巧分类"></a>按照技巧分类</h3><ol>
<li>贝叶斯学习：在概率模型的学习和推理中，利用贝叶斯定理，计算在给定数据条件下模型的条件概率，并应用这个定理进行模型的估计以及数据的预测。</li>
<li>核方法：使用核函数表示和学习非线性模型的一种机器学习方法。</li>
</ol>
<h2 id="统计学习方法三要素"><a href="#统计学习方法三要素" class="headerlink" title="统计学习方法三要素"></a>统计学习方法三要素</h2><h3 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h3><p>模型就是要学习的条件概率分布或者决策函数</p>
<h3 id="策略"><a href="#策略" class="headerlink" title="策略"></a>策略</h3><p>策略就是学习或者选择模型的准则</p>
<ol>
<li>损失函数度量模型一次预测的好坏</li>
<li>风险函数度量平均意义下模型预测的好坏，也称为期望损失</li>
<li>期望风险是模型关于联合分布的的期望损失</li>
<li>经验风险是模型关于训练样本集的平均损失</li>
<li>结构风险是在经验风险上加上表示模型复杂度的正则化项或罚项</li>
<li>训练误差是指模型关于训练集的平均损失</li>
<li>测试误差是指模型关于测试集的平均损失</li>
<li>过拟合是指学习时选择的模型所包含的参数过多，以至于模型对已知的数据预测的很好，而对未知的数据预测的很差的现象。</li>
<li>交叉验证：<ul>
<li>简单交叉验证（流出交叉验证）：        <ul>
<li>随机将数据分为两部分，一部分用于训练集，一部分用于验证集。</li>
</ul>
</li>
<li>K折交叉验证：<ul>
<li>随机将数据切分为K个互不交互，大小相等的数据集，每次用其中的K-1个数据集进行训练，用余下的子集测试模型，这样进行K次，用K次的平均值作为模型评价指标</li>
</ul>
</li>
<li>留一交叉验证：<ul>
<li>K折交叉验证的特例，取K折数等于数据集容量N</li>
</ul>
</li>
</ul>
</li>
<li>泛化误差：模型对未知数据预测的误差的期望</li>
<li>生成模型/判别模型：<ul>
<li>生成模型通过数据学习联合概率分布 $P(X,Y)$ 然后求出条件概率分布 $P(X|Y)$ 作为预测的模型。它表示了给定输入$X$输出 $Y$的生成关系。</li>
<li>判别模型通过数据直接学习决策函数 $f(X)$ 或者条件概率分布 $P(X|Y)$ 作为预测模型。它关心的是对给定的输入$X$，应该预测什么样的输出$Y$<h3 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h3>算法就是学习模型的具体计算方法</li>
</ul>
</li>
</ol>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2019/05/28/%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E3%80%8B%E7%AC%94%E8%AE%B0--%E6%A6%82%E8%BF%B0/" data-id="ckm1rujxx001c88tsh60m2r01" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag">机器学习</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0/" rel="tag">统计学习</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-社会工程学简述" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/05/09/%E7%A4%BE%E4%BC%9A%E5%B7%A5%E7%A8%8B%E5%AD%A6%E7%AE%80%E8%BF%B0/" class="article-date">
  <time datetime="2019-05-09T08:49:37.000Z" itemprop="datePublished">2019-05-09</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B8%8E%E7%BD%91%E7%BB%9C%E5%AE%89%E5%85%A8/">大数据与网络安全</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/05/09/%E7%A4%BE%E4%BC%9A%E5%B7%A5%E7%A8%8B%E5%AD%A6%E7%AE%80%E8%BF%B0/">社会工程学学习笔记概述</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="社会工程学学习笔记概述"><a href="#社会工程学学习笔记概述" class="headerlink" title="社会工程学学习笔记概述"></a>社会工程学学习笔记概述</h1><h2 id="什么是社会工程学"><a href="#什么是社会工程学" class="headerlink" title="什么是社会工程学"></a>什么是社会工程学</h2><blockquote>
<p>广义的说，社会工程学的定义是：通过建立理论并通过自然的，社会和制度上的途径且特别强调根据现实的双向计划和设计经验来一步一步的解决各种社会问题。在网络安全领域则更多的指的是通过欺骗，欺诈来操纵他人采取特定行动或泄露机密信息的行为。</p>
</blockquote>
<h2 id="社会工程攻击"><a href="#社会工程攻击" class="headerlink" title="社会工程攻击"></a>社会工程攻击</h2><p>总览整个计算机领域的社会工程学攻击，大体上可以分为两大部分。</p>
<ul>
<li>其一为非接触的信息收集，通过各种手段收集尽可能多的攻击目标的信息，提高后续攻击成功的可能性。</li>
<li>其二为与人交流的社会工程学攻击，通过在于相关人员交流的过程中套取感兴趣的信息，或者操纵有关人员实施某些特定的行为。</li>
</ul>
<h3 id="常见攻击方式"><a href="#常见攻击方式" class="headerlink" title="常见攻击方式"></a>常见攻击方式</h3><ol>
<li>结合实际环境渗透<br>针对特定环境进行渗透攻击，通常会根据观察被攻击者的一些日常行为习惯，如邮件的使用频率，重视程度，社交软件的使用频率和时间段等信息，结合获取的一些个人信息来综合判断，从而获取敏感信息甚至猜解出被攻击者的账户密码等重要信息。</li>
<li>伪装欺骗被攻击者<br>电子邮件伪造攻击，网络钓鱼攻击等攻击手法都可以伪造欺骗被攻击者，从而使其进入指定的页面或者进行特定的操作。黑客主要利用被攻击者的猎奇，贪婪，疏于防范的心理引诱用户进而实现伪装欺骗的目的。</li>
<li>说服被攻击者<br>说服是对网络安全影响较大的一种社工攻击手法。它要求黑客与被攻击者达成某种一致，进而使被攻击者主动为黑客攻击过程提供某种便利，如在僵尸网络的发展传播过程中就有主动志愿成为“肉鸡”的主机，从而从BotMaster手中获取一定利益的被攻击者。当被攻击者的利益与黑客没有冲突甚至是一致使，这种手段会非常有效。</li>
<li>恐吓被攻击者<br>黑客在开展社工攻击过程中，常常会利用被攻击目标管理人员对目标系统安全的敏感性，以权威机构的身份出现，散步以安全警告，系统风险等恐吓性的手段迫使管理人员进行制定的操作，进而实现对目标信息的获取</li>
<li>反向社会工程学<br>反向社会工程学是指黑客通过技术或非技术手段给网络或者计算机造成故障，使得被攻击者深信问题的存在，诱使工作人员主动将信息提供给黑客。</li>
</ol>
<h3 id="常见的信息搜集"><a href="#常见的信息搜集" class="headerlink" title="常见的信息搜集"></a>常见的信息搜集</h3><ol>
<li>爬虫<br>爬虫是按照一定规则自动抓取网络信息的程序或脚本，网络搜索引擎就是某种形式的爬虫，我们可以通过爬虫来获取和关键词有关的信息并进行下载。</li>
<li>搜索引擎语法<br>我们可以使用搜索引擎专用的各种语法来进行准确高效的搜索，常见的语法有“*”替代符，“inurl”搜索包含特定字符的URL，“intitle”搜索网页标题中包含有特定字符的网页，“filetype”搜索指定类型的文件。</li>
<li>Nmap（网络映射器）<br>Namp是一款用于网络发现和安全审计的网络安全工具。它可以枚举网络主机清单，监控主机或服务运行状态，探测端口开放情况等。</li>
<li>DNS分析<br>使用DNS分析可以收集有关DNS服务器和测试目标的相应记录信息。常用的有DNSenum，Maltego等。</li>
<li>Nessus<br>Nessus是世界上最流行的漏洞扫描程序，它支持同时在本机或远端上遥控，进行系统的漏洞分析扫描,是社会工程学攻击中必不可少的工具。<h3 id="其他方面的攻击"><a href="#其他方面的攻击" class="headerlink" title="其他方面的攻击"></a>其他方面的攻击</h3><h4 id="恶意插件"><a href="#恶意插件" class="headerlink" title="恶意插件"></a>恶意插件</h4>浏览器恶意插件主要是通过引诱用户安装有一定功能的恶意插件，从而盗取用户的Cookie信息进行CSRF攻击（跨站请求伪造攻击）<h4 id="敲诈勒索软件"><a href="#敲诈勒索软件" class="headerlink" title="敲诈勒索软件"></a>敲诈勒索软件</h4>按照危害程度可以分为影响使用，恐吓用户，绑架数据这三类。一般都是通过伪装成正常软件来诱使用户下载，当用户下载运行之后就直接执行指定的操作。<h2 id="社会工程学攻击的防范"><a href="#社会工程学攻击的防范" class="headerlink" title="社会工程学攻击的防范"></a>社会工程学攻击的防范</h2></li>
<li><strong>保护个人信息资料不外泄</strong><br>在网络上注册信息时需要查看网站是否提供了对个人隐私信息的保护功能，尽可能的不要使用真实信息，提高注册过程中使用密码的复杂程度，以防止个人信息被黑客暴力破解。</li>
<li><strong>时刻提高警惕</strong><br>在网络环境中，利用社会工程学攻击的手段复杂多变，网页的造伪很容易实现，收发的邮件中的收件人的地址也很容易造伪，因此不要轻易相信网络中看到的信息。</li>
<li><strong>保持理性思维</strong><br>很多黑客在攻击时利用的方式大多数是人感性的弱点，进而施加影响。我们在与陌生人沟通时，应尽量保持理性思维，减少上当受骗的概率。</li>
<li><strong>不要随意丢弃废物</strong><br>日常生活中我们的很多废弃物中都包含很多个人信息，如发票，取款机凭证等，这些看似无用的信息可能被有心的黑客利用实施社会工程学攻击，因此在丢弃废弃物时应小心谨慎，将其完全销毁在丢弃到垃圾桶中，防止个人信息的泄露。</li>
</ol>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2019/05/09/%E7%A4%BE%E4%BC%9A%E5%B7%A5%E7%A8%8B%E5%AD%A6%E7%AE%80%E8%BF%B0/" data-id="ckm1rujyc002888ts8xza13ui" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E5%AF%86%E7%A0%81%E6%94%BB%E5%87%BB/" rel="tag">密码攻击</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%B8%97%E9%80%8F%E6%B5%8B%E8%AF%95/" rel="tag">渗透测试</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-Android自动化测试" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/11/09/Android%E8%87%AA%E5%8A%A8%E5%8C%96%E6%B5%8B%E8%AF%95/" class="article-date">
  <time datetime="2018-11-09T08:49:37.000Z" itemprop="datePublished">2018-11-09</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Android/">Android</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/11/09/Android%E8%87%AA%E5%8A%A8%E5%8C%96%E6%B5%8B%E8%AF%95/">Android自动化测试</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="Android自动化测试"><a href="#Android自动化测试" class="headerlink" title="Android自动化测试"></a>Android自动化测试</h1><blockquote>
<ol>
<li>adb shell monkey -p package [事件数]</li>
</ol>
</blockquote>
<p>用于对指定包名的应用进行压力测试</p>
<blockquote>
<ol>
<li>adb logcat | find “标签字符串”</li>
</ol>
</blockquote>
<p>用于将logcat中的命令输出到控制台</p>
<blockquote>
<ol>
<li>adb shell monkey —throttle [milliseconds]</li>
</ol>
</blockquote>
<p>每步操作之间间隔时长为 milliseconds</p>
<blockquote>
<ol>
<li>adb shell monkey -s seed [event-count]</li>
</ol>
</blockquote>
<p>指定一个随机生成的值，每一个随机值确定一组相同的随机操作</p>
<blockquote>
<ol>
<li>adb shell monkey —ptc-touch [percent]</li>
</ol>
</blockquote>
<p>设定一组monkey事件组中触摸事件所占百分比</p>
<blockquote>
<ol>
<li>adb shell monkey —ptc-motion [percent]</li>
</ol>
</blockquote>
<p>设定一组monkey事件组中动作事件所占百分比</p>
<blockquote>
<ol>
<li>adb shell monkey —ptc-trackball [percent]</li>
</ol>
</blockquote>
<p>设定一组monkey事件组中轨迹球事件所占百分比</p>
<blockquote>
<ol>
<li>adb shell monkey —ptc-nav [percent]</li>
</ol>
</blockquote>
<p>设定一组monkey事件组中基本导航事件所占百分比</p>
<blockquote>
<ol>
<li>adb shell monkey —ptc-majornav [percent]</li>
</ol>
</blockquote>
<p>设定一组monkey事件组中主要导航事件所占百分比</p>
<blockquote>
<ol>
<li>adb shell monkey —ptc-syskeys [percent]</li>
</ol>
</blockquote>
<p>设定一组monkey事件组中系统导航事件所占百分比</p>
<blockquote>
<ol>
<li>adb shell monkey —ptc-appswitch [percent]</li>
</ol>
</blockquote>
<p>设定一组monkey事件组中启动Activity事件所占百分比</p>
<blockquote>
<ol>
<li>adb shell monkey —ptc-anyevent [percent]</li>
</ol>
</blockquote>
<p>设定一组monkey事件组中不常用事件所占百分比</p>
<blockquote>
<ol>
<li>adb shell monkey —ignore-crashs [event-count]</li>
</ol>
</blockquote>
<p>忽略崩溃事件继续进行测试</p>
<blockquote>
<ol>
<li>adb shell monkey —ignore-timeout [event-count]</li>
</ol>
</blockquote>
<p>忽略ANR事件继续进行测试</p>
<blockquote>
<ol>
<li>adb shell monkey -f [scriptfile] [event-count] </li>
</ol>
</blockquote>
<p>执行monkey脚本</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/11/09/Android%E8%87%AA%E5%8A%A8%E5%8C%96%E6%B5%8B%E8%AF%95/" data-id="ckm1rujx5000188ts26tq60fc" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Android/" rel="tag">Android</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E8%87%AA%E5%8A%A8%E5%8C%96%E6%B5%8B%E8%AF%95/" rel="tag">自动化测试</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-CCNA考试复习总结" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/11/09/CCNA%E8%80%83%E8%AF%95%E5%A4%8D%E4%B9%A0%E6%80%BB%E7%BB%93/" class="article-date">
  <time datetime="2018-11-09T08:49:37.000Z" itemprop="datePublished">2018-11-09</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B8%8E%E7%BD%91%E7%BB%9C%E5%AE%89%E5%85%A8/">大数据与网络安全</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/11/09/CCNA%E8%80%83%E8%AF%95%E5%A4%8D%E4%B9%A0%E6%80%BB%E7%BB%93/">CCNA考试复习总结</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="CCNA考试复习总结"><a href="#CCNA考试复习总结" class="headerlink" title="CCNA考试复习总结"></a>CCNA考试复习总结</h1><ul>
<li><p>设置默认网关：ip default-network [网关IP地址]或者是：ip route [源ip地址]  [ 子 网 掩码] [下一跳ip地址]</p>
</li>
<li><p>为需要远程管理的交换机进行配置：</p>
<ul>
<li>ip default-getway [网关地址]</li>
<li>interface vlan 1</li>
<li>ip address [需要远程管理的交换机的ip] [掩码]</li>
<li>no shutdown</li>
</ul>
</li>
<li><p>网络不可连接时的检查步骤：</p>
<ul>
<li>检查网络连接是否正常</li>
<li>验证网卡配置</li>
<li>验证IP配置</li>
<li>检查URL是否正确</li>
</ul>
</li>
<li><p>四种不同的线：</p>
<ul>
<li>同种设备使用交叉反线（crossover）</li>
<li>异种设备间使用直通线（straight through）</li>
<li>PC的COM口连接交换机的console口使用全反电缆（rollover cable）</li>
<li>连接带有串口的串行线（serial）</li>
</ul>
</li>
<li><p>名词解释：</p>
<ul>
<li>CIR：约定信息速率</li>
<li>DCE：数据通讯设备</li>
<li>DTE：数据终端设备</li>
<li>LMI：本地管理接口</li>
<li>PVC：永久虚拟电路</li>
<li>SVC：虚电路</li>
<li>DLCI：数据链路标识符</li>
<li>PVST+：增强的按VLAN生成树</li>
</ul>
</li>
<li><p>几种协议的管理距离：</p>
<ul>
<li>RIP：120</li>
<li>OSPF：110</li>
<li>静态路由：1</li>
<li>内部EIGRP：90</li>
<li>外部EIGRP：170</li>
<li>内部BGP：200</li>
<li>EGP：140</li>
<li>EIGRP路由汇总：5</li>
<li>外部BGP：20</li>
<li>IGRP：100</li>
<li>IS-IS自制系统：115</li>
<li>EGP：140</li>
<li>直连网络：0</li>
</ul>
</li>
<li><p>组播地址：</p>
<ul>
<li>IPV4：224.0.0.2</li>
<li>IPV6：FF02：：2</li>
</ul>
</li>
<li><p>本地地址：</p>
<ul>
<li>IPV4:127.0.0.1</li>
<li>IPV6： FE80开头 + {本地的MAC地址}</li>
</ul>
</li>
<li><p>查看CPU使用率：show process</p>
</li>
<li><p>查看trunk端口：</p>
<ul>
<li>show interface trunk</li>
<li>show interface switchport</li>
</ul>
</li>
<li><p>交换机在trunk模式下的模式：</p>
<ul>
<li>auto：不会主动发送DTP信息</li>
<li>on：强制称为trunk，也会主动发送信息</li>
<li>desireable：DTP主动模式，发DTP和对方协商</li>
</ul>
</li>
<li><p>STP（802.1d）端口状态：</p>
<ul>
<li>阻塞(blocking)该端口被阻塞，不可以转发或接收数据包</li>
<li>监听(listening)该端口正在等待接收bpdu数据包，bpdu可能告知该端口重新回到阻塞状态</li>
<li>学习(learning)该端口正在向其转发数据库中添加地址，但是，并不转发数据包</li>
<li>转发(forwarding)该端口正在转发数据包</li>
<li>失效(disabled)该端口只是相应网管消息，并且必须先转到阻塞状态</li>
</ul>
</li>
<li><p>RSTP（802.1w）端口状态：</p>
<ul>
<li>禁止（Discarding）</li>
<li>学习（Learning）</li>
<li>转发（Forwarding）</li>
</ul>
</li>
<li><p>IPV6任播的三个特点：</p>
<ul>
<li>数据包传送到该组接口，转发到最近结点</li>
<li>同组中多个接口公用一个地址</li>
<li>发送到任播地址的接口被发送到最近的结点</li>
</ul>
</li>
<li><p>Trunk的协议：</p>
<ul>
<li>通用的802.1q</li>
<li>思科私有的ISL</li>
</ul>
</li>
<li><p>IPV4向IPV6转换的方式：</p>
<ul>
<li>隧道</li>
<li>双栈</li>
<li>NAT-PT</li>
</ul>
</li>
<li><p>DoD模型：</p>
<ul>
<li>Application</li>
<li>Host to Host</li>
<li>Internet</li>
<li>Network Access</li>
</ul>
</li>
<li><p>查看OSPF链路状态：show ip ospf database </p>
</li>
<li><p>OSPF中进程号范围为1-65535</p>
</li>
<li><p>DHCP中IP地址冲突解决方案：检测到IP地址冲突时，会将冲突的IP地址删除，直到冲突解决之后再将IP地址放回</p>
</li>
<li><p>查看端口安全状态：show port-security interface [端口名]</p>
</li>
<li><p>链路状态协议的特点：</p>
<ul>
<li>提供参看拓扑的命令</li>
<li>计算最短路径<br>-利用触发更新</li>
</ul>
</li>
<li><p>OSPF建立邻居关系的四个条件：</p>
<ul>
<li>区域ID一致</li>
<li>hello，dead时间一致</li>
<li>认证方式和认证密码一致</li>
<li>区域性质一致（都是普通区域或者是末节区域）</li>
</ul>
</li>
<li><p>在全局配置模式下查看直连设备：cdp run</p>
</li>
<li><p>EIGRP查看邻居关系：show ip eigrp neighbors</p>
</li>
<li><p>PAP采用两次握手机制，CHAP采用三次握手机制</p>
</li>
<li><p>PVC状态：</p>
<ul>
<li>ACTIVE：成功的端对端电路</li>
<li>INACTIVE：表示成功连接到交换机，但是在另一端未检测到DTE</li>
<li>配置的端口被交换机视为无效</li>
</ul>
</li>
<li><p>SNMPv3比SNMPv2多添加的：</p>
<ul>
<li>数据完整性</li>
<li>认证</li>
<li>加密</li>
</ul>
</li>
<li><p>系统日志存储位置：</p>
<ul>
<li>RAM</li>
<li>控制台终端</li>
<li>系统日志服务器</li>
</ul>
</li>
<li><p>系统日志级别：</p>
<ul>
<li>0：Emergency</li>
<li>1：Alert</li>
<li>2：Critical</li>
<li>3：Error</li>
<li>4：Warning</li>
<li>5：Notice</li>
<li>6：Informational</li>
<li>7：Debug</li>
</ul>
</li>
<li><p>HSRP的virtual mac address以0000.0c07.acxx开头</p>
</li>
<li><p>IPV6任播的三个特点：</p>
<ul>
<li>数据包转发到该组接口，转发到最近的结点</li>
<li>同组中多个接口公用一个地址</li>
<li>发到任播地址的包被转发到最近的结点</li>
</ul>
</li>
<li><p>IPV6地址：</p>
<ul>
<li>FF01::1 — all nodes（node-local）</li>
<li>FF01::2 — all routers(node-local)</li>
<li>FF02::1 — all nodes(link-local)</li>
<li>FF02::2 — all routers(link-local)</li>
<li>FF02::5 — OSPFv3 routers</li>
<li>FF02::6 — OSPFv3 designated routers</li>
<li>FF02::9 — RIP</li>
<li>FF02::A — EIGRP routers</li>
<li>FF02::B — Mobile agents</li>
<li>FF02::C — SSDP</li>
<li>FF02::D — all PIM routers</li>
<li>FF05::2 — all routers(site-local)</li>
<li>FF05::1:3 — DHCP</li>
</ul>
</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/11/09/CCNA%E8%80%83%E8%AF%95%E5%A4%8D%E4%B9%A0%E6%80%BB%E7%BB%93/" data-id="ckm1rujx9000388ts16tcdgk1" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/CCNA/" rel="tag">CCNA</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Cisco/" rel="tag">Cisco</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/" rel="tag">计算机网络</a></li></ul>

    </footer>
  </div>
  
</article>


  


  <nav id="page-nav">
    
    <a class="extend prev" rel="prev" href="/">&amp;laquo; 上一页</a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><a class="page-number" href="/page/4/">4</a><a class="extend next" rel="next" href="/page/3/">下一页 &amp;raquo;</a>
  </nav>

</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">分类</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Android/">Android</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B8%8E%E7%BD%91%E7%BB%9C%E5%AE%89%E5%85%A8/">大数据与网络安全</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/">数据结构与算法</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%9D%82%E5%AD%A6/">杂学</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/">编程语言</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">标签</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/Android/" rel="tag">Android</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/CCNA/" rel="tag">CCNA</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Cisco/" rel="tag">Cisco</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ELK/" rel="tag">ELK</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Java/" rel="tag">Java</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Material-Design/" rel="tag">Material Design</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Matplotlib/" rel="tag">Matplotlib</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/MongoDB/" rel="tag">MongoDB</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/OpenSOC/" rel="tag">OpenSOC</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Python/" rel="tag">Python</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Seaborn/" rel="tag">Seaborn</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/UI%E6%94%AF%E6%8C%81%E5%BA%93/" rel="tag">UI支持库</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Windows/" rel="tag">Windows</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E4%BF%A1%E6%81%AF%E6%94%B6%E9%9B%86/" rel="tag">信息收集</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%9B%BE/" rel="tag">图</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%AF%86%E7%A0%81%E6%94%BB%E5%87%BB/" rel="tag">密码攻击</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%B0%8F%E7%9F%A5%E8%AF%86/" rel="tag">小知识</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/" rel="tag">排序算法</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96/" rel="tag">数据可视化</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/" rel="tag">数据库</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/" rel="tag">数据结构</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%97%A5%E5%BF%97%E5%88%86%E6%9E%90/" rel="tag">日志分析</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag">机器学习</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%A0%91/" rel="tag">树</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%B8%97%E9%80%8F%E6%B5%8B%E8%AF%95/" rel="tag">渗透测试</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%88%AC%E8%99%AB/" rel="tag">爬虫</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%AE%97%E6%B3%95/" rel="tag">算法</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%B3%BB%E7%BB%9F%E5%AE%89%E5%85%A8%E6%A6%82%E8%BF%B0/" rel="tag">系统安全概述</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0/" rel="tag">统计学习</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%BC%96%E7%A8%8B%E8%A7%84%E8%8C%83/" rel="tag">编程规范</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%BD%91%E7%BB%9C%E5%8D%8F%E8%AE%AE/" rel="tag">网络协议</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E8%87%AA%E5%8A%A8%E5%8C%96%E6%B5%8B%E8%AF%95/" rel="tag">自动化测试</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/" rel="tag">计算机网络</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">标签云</h3>
    <div class="widget tagcloud">
      <a href="/tags/Android/" style="font-size: 10px;">Android</a> <a href="/tags/CCNA/" style="font-size: 10px;">CCNA</a> <a href="/tags/Cisco/" style="font-size: 10px;">Cisco</a> <a href="/tags/ELK/" style="font-size: 10px;">ELK</a> <a href="/tags/Java/" style="font-size: 10px;">Java</a> <a href="/tags/Material-Design/" style="font-size: 10px;">Material Design</a> <a href="/tags/Matplotlib/" style="font-size: 12.5px;">Matplotlib</a> <a href="/tags/MongoDB/" style="font-size: 10px;">MongoDB</a> <a href="/tags/OpenSOC/" style="font-size: 10px;">OpenSOC</a> <a href="/tags/Python/" style="font-size: 10px;">Python</a> <a href="/tags/Seaborn/" style="font-size: 15px;">Seaborn</a> <a href="/tags/UI%E6%94%AF%E6%8C%81%E5%BA%93/" style="font-size: 10px;">UI支持库</a> <a href="/tags/Windows/" style="font-size: 10px;">Windows</a> <a href="/tags/%E4%BF%A1%E6%81%AF%E6%94%B6%E9%9B%86/" style="font-size: 10px;">信息收集</a> <a href="/tags/%E5%9B%BE/" style="font-size: 10px;">图</a> <a href="/tags/%E5%AF%86%E7%A0%81%E6%94%BB%E5%87%BB/" style="font-size: 12.5px;">密码攻击</a> <a href="/tags/%E5%B0%8F%E7%9F%A5%E8%AF%86/" style="font-size: 10px;">小知识</a> <a href="/tags/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/" style="font-size: 10px;">排序算法</a> <a href="/tags/%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96/" style="font-size: 17.5px;">数据可视化</a> <a href="/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/" style="font-size: 10px;">数据库</a> <a href="/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/" style="font-size: 12.5px;">数据结构</a> <a href="/tags/%E6%97%A5%E5%BF%97%E5%88%86%E6%9E%90/" style="font-size: 10px;">日志分析</a> <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" style="font-size: 20px;">机器学习</a> <a href="/tags/%E6%A0%91/" style="font-size: 10px;">树</a> <a href="/tags/%E6%B8%97%E9%80%8F%E6%B5%8B%E8%AF%95/" style="font-size: 15px;">渗透测试</a> <a href="/tags/%E7%88%AC%E8%99%AB/" style="font-size: 10px;">爬虫</a> <a href="/tags/%E7%AE%97%E6%B3%95/" style="font-size: 10px;">算法</a> <a href="/tags/%E7%B3%BB%E7%BB%9F%E5%AE%89%E5%85%A8%E6%A6%82%E8%BF%B0/" style="font-size: 10px;">系统安全概述</a> <a href="/tags/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0/" style="font-size: 20px;">统计学习</a> <a href="/tags/%E7%BC%96%E7%A8%8B%E8%A7%84%E8%8C%83/" style="font-size: 10px;">编程规范</a> <a href="/tags/%E7%BD%91%E7%BB%9C%E5%8D%8F%E8%AE%AE/" style="font-size: 10px;">网络协议</a> <a href="/tags/%E8%87%AA%E5%8A%A8%E5%8C%96%E6%B5%8B%E8%AF%95/" style="font-size: 10px;">自动化测试</a> <a href="/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/" style="font-size: 12.5px;">计算机网络</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">归档</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/03/">三月 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/08/">八月 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/07/">七月 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/06/">六月 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/05/">五月 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/11/">十一月 2018</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">最新文章</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2021/03/09/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E6%80%BB%E7%BB%93/">(no title)</a>
          </li>
        
          <li>
            <a href="/2019/08/01/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">Matplotlib学习笔记</a>
          </li>
        
          <li>
            <a href="/2019/08/01/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%882%EF%BC%89--Matplotlib%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1%E7%BB%98%E5%9B%BE/">Matplotlib学习笔记（二）</a>
          </li>
        
          <li>
            <a href="/2019/08/01/Python%E5%B0%8F%E7%9F%A5%E8%AF%86%E6%B1%87%E6%80%BB/">Python小知识汇总</a>
          </li>
        
          <li>
            <a href="/2019/08/01/Seaborn%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">Seaborn学习笔记</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2021 WeiQingcai<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>




<script src="/js/script.js"></script>




  </div>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>